{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilsGAN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/88/xvwxrft15lq2lkj7lv12yvvw0000gn/T/ipykernel_23487/247944773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutilsGAN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import seaborn as sns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utilsGAN'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import pprint\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv3D, Conv2D, Conv1D, Convolution2D, Deconvolution2D, Cropping2D, UpSampling2D\n",
    "from keras.layers import Input, Conv2DTranspose, ConvLSTM2D, TimeDistributed, Embedding\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Concatenate, concatenate, Reshape, multiply, ZeroPadding2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.layers import Input, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, AveragePooling2D, UpSampling2D, Reshape, core, Dropout, LeakyReLU\n",
    "import keras.backend as kb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GANModel():\n",
    "    def __init__(self, batch_size=32, inputShape=(128, 128, 3), dropout_prob=0.25): \n",
    "        self.batch_size = batch_size\n",
    "        self.inputShape = inputShape\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "        \n",
    "  \n",
    "        #discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5),metrics=['accuracy'])\n",
    " \n",
    "        #generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        first_frame = Input(shape=self.inputShape)\n",
    "        last_frame = Input(shape=self.inputShape)\n",
    "\n",
    "        # By conditioning on the first frame generate a fake version of the last frame\n",
    "        fake_last_frame = self.generator(first_frame)\n",
    "\n",
    "        # Only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        valid = self.discriminator([fake_last_frame, first_frame])\n",
    "\n",
    "        self.combined = Model(inputs=[last_frame, first_frame], outputs=[valid, fake_last_frame])\n",
    "        self.combined.compile(loss='binary_crossentropy',\n",
    "                              optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "        inputs = Input(shape=self.inputShape)\n",
    "        momentum_rate = 0.8\n",
    "        \n",
    "        \n",
    "        def conv2d(layer_input, filters, f_size=3, bn=False, double=False,dropout_rate=0 ):\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            \n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if dropout_rate:\n",
    "                d = Dropout(dropout_rate)(d)\n",
    "            if double:\n",
    "                d = Conv2D(filters, kernel_size=f_size,strides=1, padding='same')(d)\n",
    "                d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, n_filters, f_size=3, dropout_rate=0, double=False,bn=True):\n",
    "            u = Conv2DTranspose(n_filters,kernel_size=3, strides=2, padding='same')(layer_input)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            \n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = LeakyReLU(alpha=0.2)(u)\n",
    "                        \n",
    "            if double:\n",
    "                u = Conv2D(n_filters, kernel_size=f_size,strides=1, padding='same')(u)\n",
    "                u = LeakyReLU(alpha=0.2)(u)\n",
    "            return u\n",
    "        \n",
    "        \n",
    "        start = Conv2D(self.gf, 3, padding='same')(inputs)\n",
    "        start = LeakyReLU(alpha=0.2)(start)\n",
    "        d1 = conv2d(start, self.gf*2)#64\n",
    "        d2 = conv2d(d1, self.gf*4,bn=True, dropout_rate=0.5)\n",
    "        d3 = conv2d(d2, self.gf*8,)\n",
    "        d4 = conv2d(d3, self.gf*8,bn=True, dropout_rate=0.5)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        u2 = deconv2d(d6, d5, self.gf*8, bn=False)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*8)\n",
    "        u5 = deconv2d(u4, d2, self.gf*4)\n",
    "        u6 = deconv2d(u5, d1, self.gf*2)\n",
    "        u8= deconv2d(u6, start, self.gf)\n",
    "        \n",
    "        nbr_img_channels = 3\n",
    "        classify = Conv2D(nbr_img_channels, (1, 1), activation='sigmoid')(u8)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=classify, name='Generator')\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        last_img = Input(shape=self.inputShape)\n",
    "        first_img = Input(shape=self.inputShape)\n",
    "        alpha=0.2\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([last_img, first_img])\n",
    "\n",
    "        d2 = Conv2D(self.df, 3, strides=2, padding='same')(combined_imgs) \n",
    "        d2 = LeakyReLU(alpha=alpha)(d2)\n",
    "        d2 = Dropout(0.5)(d2)\n",
    "        \n",
    "        d2 = Conv2D(self.df*2, 3, strides=2, padding='same')(d2)\n",
    "        d2 = LeakyReLU(alpha=alpha)(d2)\n",
    "        d2 = Dropout(0.5)(d2)\n",
    "  \n",
    "        d2 = Conv2D(self.df*4, 3, strides=2, padding='same')(d2)\n",
    "        d2 = LeakyReLU(alpha=alpha)(d2)\n",
    "\n",
    "               \n",
    "        validity = Conv2D(1, (3, 3), strides=2, padding='same', activation='tanh')(d2)\n",
    "        \n",
    "\n",
    "        model = Model([last_img, first_img], validity)\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/88/xvwxrft15lq2lkj7lv12yvvw0000gn/T/ipykernel_24812/4081177595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_CHANNEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodelObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGANModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputShape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDROPOUT_PROB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#modelObj = DCGAN()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "image_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL)\n",
    "modelObj = GANModel(batch_size=cfg.BATCH_SIZE, inputShape=image_shape, dropout_prob=cfg.DROPOUT_PROB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "# Adversarial loss ground truths\n",
    "valid = np.ones((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "fake = np.zeros((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "# log file\n",
    "output_log_dir = \"./logs/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(output_log_dir):\n",
    "    os.makedirs(output_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "(50, 128, 128, 3)\n",
      "0.5046887\n",
      "[Epoch 0/3] [Batch 0/216] [D loss: 0.503479] [G loss: 51.701847] time: 0:00:02.623190\n",
      "(50, 128, 128, 3)\n",
      "0.50784343\n",
      "[Epoch 0/3] [Batch 1/216] [D loss: 0.320332] [G loss: 46.896183] time: 0:00:03.195715\n",
      "(50, 128, 128, 3)\n",
      "0.5165524\n",
      "[Epoch 0/3] [Batch 2/216] [D loss: 0.281362] [G loss: 42.864548] time: 0:00:03.790115\n",
      "(50, 128, 128, 3)\n",
      "0.52651864\n",
      "[Epoch 0/3] [Batch 3/216] [D loss: 0.276795] [G loss: 40.628166] time: 0:00:04.365665\n",
      "(50, 128, 128, 3)\n",
      "0.54154927\n",
      "[Epoch 0/3] [Batch 4/216] [D loss: 0.270919] [G loss: 37.380886] time: 0:00:04.925701\n",
      "(50, 128, 128, 3)\n",
      "0.55348235\n",
      "[Epoch 0/3] [Batch 5/216] [D loss: 0.268467] [G loss: 36.999401] time: 0:00:05.497708\n",
      "(50, 128, 128, 3)\n",
      "0.56945187\n",
      "[Epoch 0/3] [Batch 6/216] [D loss: 0.269541] [G loss: 35.645245] time: 0:00:06.061457\n",
      "(50, 128, 128, 3)\n",
      "0.5856711\n",
      "[Epoch 0/3] [Batch 7/216] [D loss: 0.270742] [G loss: 33.758957] time: 0:00:06.632250\n",
      "(50, 128, 128, 3)\n",
      "0.59759766\n",
      "[Epoch 0/3] [Batch 8/216] [D loss: 0.267095] [G loss: 35.126431] time: 0:00:07.210610\n",
      "(50, 128, 128, 3)\n",
      "0.61298037\n",
      "[Epoch 0/3] [Batch 9/216] [D loss: 0.262603] [G loss: 35.872173] time: 0:00:07.778270\n",
      "(50, 128, 128, 3)\n",
      "0.6279808\n",
      "[Epoch 0/3] [Batch 10/216] [D loss: 0.270818] [G loss: 32.555416] time: 0:00:08.371833\n",
      "(50, 128, 128, 3)\n",
      "0.6193012\n",
      "[Epoch 0/3] [Batch 11/216] [D loss: 0.269342] [G loss: 32.996357] time: 0:00:08.934521\n",
      "(50, 128, 128, 3)\n",
      "0.6439219\n",
      "[Epoch 0/3] [Batch 12/216] [D loss: 0.262056] [G loss: 32.292419] time: 0:00:09.510570\n",
      "(50, 128, 128, 3)\n",
      "0.6382701\n",
      "[Epoch 0/3] [Batch 13/216] [D loss: 0.263212] [G loss: 32.145634] time: 0:00:10.076918\n",
      "(50, 128, 128, 3)\n",
      "0.6320222\n",
      "[Epoch 0/3] [Batch 14/216] [D loss: 0.265924] [G loss: 31.910646] time: 0:00:10.668651\n",
      "(50, 128, 128, 3)\n",
      "0.647988\n",
      "[Epoch 0/3] [Batch 15/216] [D loss: 0.266729] [G loss: 32.231312] time: 0:00:11.254537\n",
      "(50, 128, 128, 3)\n",
      "0.661864\n",
      "[Epoch 0/3] [Batch 16/216] [D loss: 0.260572] [G loss: 28.210533] time: 0:00:11.811593\n",
      "(50, 128, 128, 3)\n",
      "0.62912995\n",
      "[Epoch 0/3] [Batch 17/216] [D loss: 0.262621] [G loss: 29.644781] time: 0:00:12.368595\n",
      "(50, 128, 128, 3)\n",
      "0.663125\n",
      "[Epoch 0/3] [Batch 18/216] [D loss: 0.261118] [G loss: 28.538868] time: 0:00:12.954445\n",
      "(50, 128, 128, 3)\n",
      "0.6580413\n",
      "[Epoch 0/3] [Batch 19/216] [D loss: 0.259573] [G loss: 30.979420] time: 0:00:13.525992\n",
      "(50, 128, 128, 3)\n",
      "0.67342645\n",
      "[Epoch 0/3] [Batch 20/216] [D loss: 0.263782] [G loss: 29.529297] time: 0:00:14.096978\n",
      "(50, 128, 128, 3)\n",
      "0.70672923\n",
      "[Epoch 0/3] [Batch 21/216] [D loss: 0.263623] [G loss: 26.071474] time: 0:00:14.675128\n",
      "(50, 128, 128, 3)\n",
      "0.6531921\n",
      "[Epoch 0/3] [Batch 22/216] [D loss: 0.266855] [G loss: 25.340239] time: 0:00:15.256857\n",
      "(50, 128, 128, 3)\n",
      "0.6408738\n",
      "[Epoch 0/3] [Batch 23/216] [D loss: 0.257135] [G loss: 26.615124] time: 0:00:15.815600\n",
      "(50, 128, 128, 3)\n",
      "0.66221493\n",
      "[Epoch 0/3] [Batch 24/216] [D loss: 0.260291] [G loss: 27.791838] time: 0:00:16.387056\n",
      "(50, 128, 128, 3)\n",
      "0.71517515\n",
      "[Epoch 0/3] [Batch 25/216] [D loss: 0.261508] [G loss: 29.348961] time: 0:00:16.966635\n",
      "(50, 128, 128, 3)\n",
      "0.7348953\n",
      "[Epoch 0/3] [Batch 26/216] [D loss: 0.271621] [G loss: 27.317816] time: 0:00:17.544638\n",
      "(50, 128, 128, 3)\n",
      "0.76313114\n",
      "[Epoch 0/3] [Batch 27/216] [D loss: 0.269793] [G loss: 26.593220] time: 0:00:18.115127\n",
      "(50, 128, 128, 3)\n",
      "0.7216564\n",
      "[Epoch 0/3] [Batch 28/216] [D loss: 0.274224] [G loss: 26.312908] time: 0:00:18.696315\n",
      "(50, 128, 128, 3)\n",
      "0.72982544\n",
      "[Epoch 0/3] [Batch 29/216] [D loss: 0.276191] [G loss: 27.511492] time: 0:00:19.264880\n",
      "(50, 128, 128, 3)\n",
      "0.77651334\n",
      "[Epoch 0/3] [Batch 30/216] [D loss: 0.266203] [G loss: 26.255051] time: 0:00:19.867205\n",
      "(50, 128, 128, 3)\n",
      "0.68775105\n",
      "[Epoch 0/3] [Batch 31/216] [D loss: 0.267551] [G loss: 25.390924] time: 0:00:20.435222\n",
      "(50, 128, 128, 3)\n",
      "0.7387484\n",
      "[Epoch 0/3] [Batch 32/216] [D loss: 0.272332] [G loss: 25.736778] time: 0:00:21.019000\n",
      "(50, 128, 128, 3)\n",
      "0.7710738\n",
      "[Epoch 0/3] [Batch 33/216] [D loss: 0.269712] [G loss: 24.899620] time: 0:00:21.601607\n",
      "(50, 128, 128, 3)\n",
      "0.7174513\n",
      "[Epoch 0/3] [Batch 34/216] [D loss: 0.265177] [G loss: 25.064674] time: 0:00:22.169354\n",
      "(50, 128, 128, 3)\n",
      "0.7168967\n",
      "[Epoch 0/3] [Batch 35/216] [D loss: 0.262061] [G loss: 24.479120] time: 0:00:22.770652\n",
      "(50, 128, 128, 3)\n",
      "0.70513123\n",
      "[Epoch 0/3] [Batch 36/216] [D loss: 0.263658] [G loss: 23.588068] time: 0:00:23.348418\n",
      "(50, 128, 128, 3)\n",
      "0.7206014\n",
      "[Epoch 0/3] [Batch 37/216] [D loss: 0.262396] [G loss: 24.693018] time: 0:00:23.922521\n",
      "(50, 128, 128, 3)\n",
      "0.7664385\n",
      "[Epoch 0/3] [Batch 38/216] [D loss: 0.258092] [G loss: 22.278582] time: 0:00:24.496643\n",
      "(50, 128, 128, 3)\n",
      "0.7000827\n",
      "[Epoch 0/3] [Batch 39/216] [D loss: 0.262075] [G loss: 23.722357] time: 0:00:25.099992\n",
      "(50, 128, 128, 3)\n",
      "0.7788048\n",
      "[Epoch 0/3] [Batch 40/216] [D loss: 0.251455] [G loss: 23.417130] time: 0:00:25.664766\n",
      "(50, 128, 128, 3)\n",
      "0.68077904\n",
      "[Epoch 0/3] [Batch 41/216] [D loss: 0.259249] [G loss: 24.209522] time: 0:00:26.234044\n",
      "(50, 128, 128, 3)\n",
      "0.79187936\n",
      "[Epoch 0/3] [Batch 42/216] [D loss: 0.267113] [G loss: 25.599068] time: 0:00:26.829706\n",
      "(50, 128, 128, 3)\n",
      "0.7212882\n",
      "[Epoch 0/3] [Batch 43/216] [D loss: 0.265439] [G loss: 23.608765] time: 0:00:27.401812\n",
      "(50, 128, 128, 3)\n",
      "0.7373244\n",
      "[Epoch 0/3] [Batch 44/216] [D loss: 0.261855] [G loss: 23.250582] time: 0:00:27.972195\n",
      "(50, 128, 128, 3)\n",
      "0.77286345\n",
      "[Epoch 0/3] [Batch 45/216] [D loss: 0.258823] [G loss: 21.817459] time: 0:00:28.545122\n",
      "(50, 128, 128, 3)\n",
      "0.72198075\n",
      "[Epoch 0/3] [Batch 46/216] [D loss: 0.254171] [G loss: 22.786358] time: 0:00:29.124773\n",
      "(50, 128, 128, 3)\n",
      "0.77860826\n",
      "[Epoch 0/3] [Batch 47/216] [D loss: 0.258863] [G loss: 20.617947] time: 0:00:29.698950\n",
      "(50, 128, 128, 3)\n",
      "0.7428162\n",
      "[Epoch 0/3] [Batch 48/216] [D loss: 0.258052] [G loss: 19.993979] time: 0:00:30.282730\n",
      "(50, 128, 128, 3)\n",
      "0.7186428\n",
      "[Epoch 0/3] [Batch 49/216] [D loss: 0.250699] [G loss: 20.859848] time: 0:00:30.873948\n",
      "(50, 128, 128, 3)\n",
      "0.71723557\n",
      "[Epoch 0/3] [Batch 50/216] [D loss: 0.257670] [G loss: 20.243208] time: 0:00:31.456517\n",
      "(50, 128, 128, 3)\n",
      "0.75505394\n",
      "[Epoch 0/3] [Batch 51/216] [D loss: 0.253178] [G loss: 20.801394] time: 0:00:32.043301\n",
      "(50, 128, 128, 3)\n",
      "0.797281\n",
      "[Epoch 0/3] [Batch 52/216] [D loss: 0.256655] [G loss: 22.254314] time: 0:00:32.615407\n",
      "(50, 128, 128, 3)\n",
      "0.7906322\n",
      "[Epoch 0/3] [Batch 53/216] [D loss: 0.264567] [G loss: 20.050352] time: 0:00:33.203939\n",
      "(50, 128, 128, 3)\n",
      "0.78328615\n",
      "[Epoch 0/3] [Batch 54/216] [D loss: 0.265401] [G loss: 23.837994] time: 0:00:33.795939\n",
      "(50, 128, 128, 3)\n",
      "0.8530595\n",
      "[Epoch 0/3] [Batch 55/216] [D loss: 0.265416] [G loss: 21.997568] time: 0:00:34.399620\n",
      "(50, 128, 128, 3)\n",
      "0.80550176\n",
      "[Epoch 0/3] [Batch 56/216] [D loss: 0.267243] [G loss: 18.780203] time: 0:00:34.974501\n",
      "(50, 128, 128, 3)\n",
      "0.763802\n",
      "[Epoch 0/3] [Batch 57/216] [D loss: 0.256438] [G loss: 20.940271] time: 0:00:35.550669\n",
      "(50, 128, 128, 3)\n",
      "0.754945\n",
      "[Epoch 0/3] [Batch 58/216] [D loss: 0.256109] [G loss: 20.721851] time: 0:00:36.139468\n",
      "(50, 128, 128, 3)\n",
      "0.7734023\n",
      "[Epoch 0/3] [Batch 60/216] [D loss: 0.258979] [G loss: 21.971107] time: 0:00:36.731971\n",
      "(50, 128, 128, 3)\n",
      "0.77606136\n",
      "[Epoch 0/3] [Batch 61/216] [D loss: 0.263438] [G loss: 21.903154] time: 0:00:37.315235\n",
      "(50, 128, 128, 3)\n",
      "0.8021144\n",
      "[Epoch 0/3] [Batch 62/216] [D loss: 0.268098] [G loss: 20.423246] time: 0:00:37.888698\n",
      "(50, 128, 128, 3)\n",
      "0.7766029\n",
      "[Epoch 0/3] [Batch 63/216] [D loss: 0.271768] [G loss: 19.957720] time: 0:00:38.468391\n",
      "(50, 128, 128, 3)\n",
      "0.79017764\n",
      "[Epoch 0/3] [Batch 64/216] [D loss: 0.267525] [G loss: 20.278917] time: 0:00:39.045511\n",
      "(50, 128, 128, 3)\n",
      "0.78289825\n",
      "[Epoch 0/3] [Batch 65/216] [D loss: 0.261396] [G loss: 18.859581] time: 0:00:39.604808\n",
      "(50, 128, 128, 3)\n",
      "0.7722628\n",
      "[Epoch 0/3] [Batch 66/216] [D loss: 0.261435] [G loss: 19.907274] time: 0:00:40.211005\n",
      "(50, 128, 128, 3)\n",
      "0.7828539\n",
      "[Epoch 0/3] [Batch 67/216] [D loss: 0.253560] [G loss: 17.693838] time: 0:00:40.801619\n",
      "(50, 128, 128, 3)\n",
      "0.7433036\n",
      "[Epoch 0/3] [Batch 68/216] [D loss: 0.248344] [G loss: 19.555510] time: 0:00:41.401260\n",
      "(50, 128, 128, 3)\n",
      "0.70705277\n",
      "[Epoch 0/3] [Batch 69/216] [D loss: 0.252688] [G loss: 18.633350] time: 0:00:41.984042\n",
      "(50, 128, 128, 3)\n",
      "0.78974575\n",
      "[Epoch 0/3] [Batch 70/216] [D loss: 0.247298] [G loss: 18.311909] time: 0:00:42.568053\n",
      "(50, 128, 128, 3)\n",
      "0.7417927\n",
      "[Epoch 0/3] [Batch 71/216] [D loss: 0.249686] [G loss: 18.901577] time: 0:00:43.158212\n",
      "(50, 128, 128, 3)\n",
      "0.76166296\n",
      "[Epoch 0/3] [Batch 72/216] [D loss: 0.254913] [G loss: 19.815289] time: 0:00:43.743670\n",
      "(50, 128, 128, 3)\n",
      "0.8071596\n",
      "[Epoch 0/3] [Batch 73/216] [D loss: 0.257036] [G loss: 19.183296] time: 0:00:44.321406\n",
      "(50, 128, 128, 3)\n",
      "0.8073947\n",
      "[Epoch 0/3] [Batch 74/216] [D loss: 0.255718] [G loss: 18.859175] time: 0:00:44.897977\n",
      "(50, 128, 128, 3)\n",
      "0.7439324\n",
      "[Epoch 0/3] [Batch 75/216] [D loss: 0.251510] [G loss: 16.886860] time: 0:00:45.475379\n",
      "(50, 128, 128, 3)\n",
      "0.7535014\n",
      "[Epoch 0/3] [Batch 76/216] [D loss: 0.253820] [G loss: 19.779167] time: 0:00:46.049034\n",
      "(50, 128, 128, 3)\n",
      "0.73357946\n",
      "[Epoch 0/3] [Batch 77/216] [D loss: 0.243402] [G loss: 18.208147] time: 0:00:46.640186\n",
      "(50, 128, 128, 3)\n",
      "0.7885799\n",
      "[Epoch 0/3] [Batch 78/216] [D loss: 0.253455] [G loss: 18.315872] time: 0:00:47.211433\n",
      "(50, 128, 128, 3)\n",
      "0.7772204\n",
      "[Epoch 0/3] [Batch 79/216] [D loss: 0.251605] [G loss: 18.683159] time: 0:00:47.801818\n",
      "(50, 128, 128, 3)\n",
      "0.79540014\n",
      "[Epoch 0/3] [Batch 80/216] [D loss: 0.248729] [G loss: 18.501366] time: 0:00:48.400896\n",
      "(50, 128, 128, 3)\n",
      "0.7799042\n",
      "[Epoch 0/3] [Batch 81/216] [D loss: 0.244246] [G loss: 18.273125] time: 0:00:48.972089\n",
      "(50, 128, 128, 3)\n",
      "0.7626078\n",
      "[Epoch 0/3] [Batch 82/216] [D loss: 0.241914] [G loss: 17.419077] time: 0:00:49.565883\n",
      "(50, 128, 128, 3)\n",
      "0.7701226\n",
      "[Epoch 0/3] [Batch 83/216] [D loss: 0.244802] [G loss: 18.159328] time: 0:00:50.141662\n",
      "(50, 128, 128, 3)\n",
      "0.76408285\n",
      "[Epoch 0/3] [Batch 84/216] [D loss: 0.248146] [G loss: 18.791142] time: 0:00:50.740489\n",
      "(50, 128, 128, 3)\n",
      "0.7902603\n",
      "[Epoch 0/3] [Batch 85/216] [D loss: 0.247242] [G loss: 17.437809] time: 0:00:51.313456\n",
      "(50, 128, 128, 3)\n",
      "0.77089256\n",
      "[Epoch 0/3] [Batch 86/216] [D loss: 0.243019] [G loss: 15.944184] time: 0:00:51.916499\n",
      "(50, 128, 128, 3)\n",
      "0.7598908\n",
      "[Epoch 0/3] [Batch 87/216] [D loss: 0.245829] [G loss: 19.961624] time: 0:00:52.498715\n",
      "(50, 128, 128, 3)\n",
      "0.85069305\n",
      "[Epoch 0/3] [Batch 88/216] [D loss: 0.246242] [G loss: 16.270857] time: 0:00:53.075470\n",
      "(50, 128, 128, 3)\n",
      "0.763494\n",
      "[Epoch 0/3] [Batch 89/216] [D loss: 0.245696] [G loss: 17.797012] time: 0:00:53.657473\n",
      "(50, 128, 128, 3)\n",
      "0.7662502\n",
      "[Epoch 0/3] [Batch 90/216] [D loss: 0.252866] [G loss: 17.346807] time: 0:00:54.256204\n",
      "(50, 128, 128, 3)\n",
      "0.81413966\n",
      "[Epoch 0/3] [Batch 91/216] [D loss: 0.248074] [G loss: 16.432163] time: 0:00:54.864836\n",
      "(50, 128, 128, 3)\n",
      "0.7473344\n",
      "[Epoch 0/3] [Batch 92/216] [D loss: 0.238772] [G loss: 17.324484] time: 0:00:55.452506\n",
      "(50, 128, 128, 3)\n",
      "0.7322407\n",
      "[Epoch 0/3] [Batch 93/216] [D loss: 0.241354] [G loss: 15.457197] time: 0:00:56.023622\n",
      "(50, 128, 128, 3)\n",
      "0.7633605\n",
      "[Epoch 0/3] [Batch 94/216] [D loss: 0.246773] [G loss: 16.638508] time: 0:00:56.590454\n",
      "(50, 128, 128, 3)\n",
      "0.79140496\n",
      "[Epoch 0/3] [Batch 95/216] [D loss: 0.231788] [G loss: 21.453974] time: 0:00:57.185377\n",
      "(50, 128, 128, 3)\n",
      "0.79084426\n",
      "[Epoch 0/3] [Batch 96/216] [D loss: 0.250010] [G loss: 18.706835] time: 0:00:57.753813\n",
      "(50, 128, 128, 3)\n",
      "0.81301475\n",
      "[Epoch 0/3] [Batch 97/216] [D loss: 0.244129] [G loss: 17.168978] time: 0:00:58.340758\n",
      "(50, 128, 128, 3)\n",
      "0.6761801\n",
      "[Epoch 0/3] [Batch 98/216] [D loss: 0.229049] [G loss: 16.484751] time: 0:00:58.916796\n",
      "(50, 128, 128, 3)\n",
      "0.8001571\n",
      "[Epoch 0/3] [Batch 99/216] [D loss: 0.237506] [G loss: 17.367802] time: 0:00:59.507593\n",
      "(50, 128, 128, 3)\n",
      "0.7736435\n",
      "[Epoch 0/3] [Batch 100/216] [D loss: 0.241951] [G loss: 16.151905] time: 0:01:00.084731\n",
      "(50, 128, 128, 3)\n",
      "0.7882545\n",
      "[Epoch 0/3] [Batch 101/216] [D loss: 0.238723] [G loss: 15.582024] time: 0:01:00.670455\n",
      "(50, 128, 128, 3)\n",
      "0.74627924\n",
      "[Epoch 0/3] [Batch 102/216] [D loss: 0.234455] [G loss: 16.888792] time: 0:01:01.235340\n",
      "(50, 128, 128, 3)\n",
      "0.80840325\n",
      "[Epoch 0/3] [Batch 103/216] [D loss: 0.244066] [G loss: 20.349144] time: 0:01:01.820551\n",
      "(50, 128, 128, 3)\n",
      "0.87839675\n",
      "[Epoch 0/3] [Batch 104/216] [D loss: 0.254609] [G loss: 16.668390] time: 0:01:02.404817\n",
      "(50, 128, 128, 3)\n",
      "0.8345695\n",
      "[Epoch 0/3] [Batch 105/216] [D loss: 0.259934] [G loss: 16.345881] time: 0:01:02.994107\n",
      "(50, 128, 128, 3)\n",
      "0.8022619\n",
      "[Epoch 0/3] [Batch 106/216] [D loss: 0.253094] [G loss: 16.455717] time: 0:01:03.581013\n",
      "(50, 128, 128, 3)\n",
      "0.8141503\n",
      "[Epoch 0/3] [Batch 107/216] [D loss: 0.249017] [G loss: 16.499310] time: 0:01:04.158323\n",
      "(50, 128, 128, 3)\n",
      "0.78161186\n",
      "[Epoch 0/3] [Batch 108/216] [D loss: 0.251470] [G loss: 16.077967] time: 0:01:04.733958\n",
      "(50, 128, 128, 3)\n",
      "0.7873497\n",
      "[Epoch 0/3] [Batch 109/216] [D loss: 0.243445] [G loss: 14.383624] time: 0:01:05.312579\n",
      "(50, 128, 128, 3)\n",
      "0.76924056\n",
      "[Epoch 0/3] [Batch 110/216] [D loss: 0.233239] [G loss: 15.626256] time: 0:01:05.894456\n",
      "(50, 128, 128, 3)\n",
      "0.7967848\n",
      "[Epoch 0/3] [Batch 111/216] [D loss: 0.238483] [G loss: 17.169859] time: 0:01:06.473821\n",
      "(50, 128, 128, 3)\n",
      "0.8105898\n",
      "[Epoch 0/3] [Batch 112/216] [D loss: 0.235789] [G loss: 19.476841] time: 0:01:07.053430\n",
      "(50, 128, 128, 3)\n",
      "0.8488371\n",
      "[Epoch 0/3] [Batch 113/216] [D loss: 0.233110] [G loss: 17.492298] time: 0:01:07.632578\n",
      "(50, 128, 128, 3)\n",
      "0.8154715\n",
      "[Epoch 0/3] [Batch 114/216] [D loss: 0.237159] [G loss: 13.872886] time: 0:01:08.233077\n",
      "(50, 128, 128, 3)\n",
      "0.6946495\n",
      "[Epoch 0/3] [Batch 115/216] [D loss: 0.211013] [G loss: 14.455155] time: 0:01:08.835067\n",
      "(50, 128, 128, 3)\n",
      "0.79174566\n",
      "[Epoch 0/3] [Batch 116/216] [D loss: 0.225598] [G loss: 15.494270] time: 0:01:09.441281\n",
      "(50, 128, 128, 3)\n",
      "0.8029426\n",
      "[Epoch 0/3] [Batch 117/216] [D loss: 0.233224] [G loss: 13.347761] time: 0:01:10.025197\n",
      "(50, 128, 128, 3)\n",
      "0.7363789\n",
      "[Epoch 0/3] [Batch 118/216] [D loss: 0.234317] [G loss: 13.311816] time: 0:01:10.604624\n",
      "(50, 128, 128, 3)\n",
      "0.7825611\n",
      "[Epoch 0/3] [Batch 119/216] [D loss: 0.226535] [G loss: 17.256832] time: 0:01:11.186084\n",
      "(50, 128, 128, 3)\n",
      "0.77910525\n",
      "[Epoch 0/3] [Batch 120/216] [D loss: 0.239335] [G loss: 14.782138] time: 0:01:11.772169\n",
      "(50, 128, 128, 3)\n",
      "0.78943056\n",
      "[Epoch 0/3] [Batch 121/216] [D loss: 0.247090] [G loss: 14.895435] time: 0:01:12.364251\n",
      "(50, 128, 128, 3)\n",
      "0.8075077\n",
      "[Epoch 0/3] [Batch 122/216] [D loss: 0.248459] [G loss: 16.025215] time: 0:01:12.942066\n",
      "(50, 128, 128, 3)\n",
      "0.846663\n",
      "[Epoch 0/3] [Batch 123/216] [D loss: 0.244291] [G loss: 13.065992] time: 0:01:13.541645\n",
      "(50, 128, 128, 3)\n",
      "0.77767855\n",
      "[Epoch 0/3] [Batch 124/216] [D loss: 0.242491] [G loss: 13.337918] time: 0:01:14.118710\n",
      "(50, 128, 128, 3)\n",
      "0.7514913\n",
      "[Epoch 0/3] [Batch 125/216] [D loss: 0.242388] [G loss: 12.528875] time: 0:01:14.694908\n",
      "(50, 128, 128, 3)\n",
      "0.77667564\n",
      "[Epoch 0/3] [Batch 126/216] [D loss: 0.242135] [G loss: 12.725229] time: 0:01:15.284915\n",
      "(50, 128, 128, 3)\n",
      "0.79364014\n",
      "[Epoch 0/3] [Batch 127/216] [D loss: 0.239171] [G loss: 12.791195] time: 0:01:15.893352\n",
      "(50, 128, 128, 3)\n",
      "0.7442598\n",
      "[Epoch 0/3] [Batch 128/216] [D loss: 0.240913] [G loss: 12.885418] time: 0:01:16.472401\n",
      "(50, 128, 128, 3)\n",
      "0.8524577\n",
      "[Epoch 0/3] [Batch 129/216] [D loss: 0.241842] [G loss: 13.927556] time: 0:01:17.062023\n",
      "(50, 128, 128, 3)\n",
      "0.7822086\n",
      "[Epoch 0/3] [Batch 130/216] [D loss: 0.239933] [G loss: 14.415469] time: 0:01:17.638225\n",
      "(50, 128, 128, 3)\n",
      "0.7492942\n",
      "[Epoch 0/3] [Batch 131/216] [D loss: 0.241923] [G loss: 12.731812] time: 0:01:18.224053\n",
      "(50, 128, 128, 3)\n",
      "0.76173085\n",
      "[Epoch 0/3] [Batch 132/216] [D loss: 0.253195] [G loss: 12.840862] time: 0:01:18.826352\n",
      "(50, 128, 128, 3)\n",
      "0.81166315\n",
      "[Epoch 0/3] [Batch 133/216] [D loss: 0.252800] [G loss: 16.144253] time: 0:01:19.405844\n",
      "(50, 128, 128, 3)\n",
      "0.7938518\n",
      "[Epoch 0/3] [Batch 134/216] [D loss: 0.252646] [G loss: 14.205979] time: 0:01:20.005967\n",
      "(50, 128, 128, 3)\n",
      "0.8140711\n",
      "[Epoch 0/3] [Batch 135/216] [D loss: 0.255647] [G loss: 14.513311] time: 0:01:20.581027\n",
      "(50, 128, 128, 3)\n",
      "0.8309167\n",
      "[Epoch 0/3] [Batch 136/216] [D loss: 0.245493] [G loss: 13.141045] time: 0:01:21.170652\n",
      "(50, 128, 128, 3)\n",
      "0.82042664\n",
      "[Epoch 0/3] [Batch 137/216] [D loss: 0.253058] [G loss: 17.086929] time: 0:01:21.753797\n",
      "(50, 128, 128, 3)\n",
      "0.8775866\n",
      "[Epoch 0/3] [Batch 138/216] [D loss: 0.251801] [G loss: 14.546401] time: 0:01:22.341233\n",
      "(50, 128, 128, 3)\n",
      "0.8025341\n",
      "[Epoch 0/3] [Batch 139/216] [D loss: 0.245700] [G loss: 13.748504] time: 0:01:22.920826\n",
      "(50, 128, 128, 3)\n",
      "0.8001577\n",
      "[Epoch 0/3] [Batch 140/216] [D loss: 0.247076] [G loss: 14.314227] time: 0:01:23.517477\n",
      "(50, 128, 128, 3)\n",
      "0.84671324\n",
      "[Epoch 0/3] [Batch 141/216] [D loss: 0.244475] [G loss: 14.563201] time: 0:01:24.118170\n",
      "(50, 128, 128, 3)\n",
      "0.83059126\n",
      "[Epoch 0/3] [Batch 142/216] [D loss: 0.255874] [G loss: 13.713644] time: 0:01:24.718006\n",
      "(50, 128, 128, 3)\n",
      "0.8657667\n",
      "[Epoch 0/3] [Batch 143/216] [D loss: 0.257929] [G loss: 13.757498] time: 0:01:25.303495\n",
      "(50, 128, 128, 3)\n",
      "0.8211408\n",
      "[Epoch 0/3] [Batch 144/216] [D loss: 0.248052] [G loss: 12.402996] time: 0:01:25.900135\n",
      "(50, 128, 128, 3)\n",
      "0.7962586\n",
      "[Epoch 0/3] [Batch 145/216] [D loss: 0.242084] [G loss: 15.124679] time: 0:01:26.506388\n",
      "(50, 128, 128, 3)\n",
      "0.8336361\n",
      "[Epoch 0/3] [Batch 146/216] [D loss: 0.241414] [G loss: 13.448465] time: 0:01:27.103474\n",
      "(50, 128, 128, 3)\n",
      "0.7643008\n",
      "[Epoch 0/3] [Batch 147/216] [D loss: 0.248622] [G loss: 11.770886] time: 0:01:27.682282\n",
      "(50, 128, 128, 3)\n",
      "0.81719416\n",
      "[Epoch 0/3] [Batch 148/216] [D loss: 0.238972] [G loss: 13.083341] time: 0:01:28.285853\n",
      "(50, 128, 128, 3)\n",
      "0.7555067\n",
      "[Epoch 0/3] [Batch 149/216] [D loss: 0.229899] [G loss: 13.848957] time: 0:01:28.884053\n",
      "(50, 128, 128, 3)\n",
      "0.8170729\n",
      "[Epoch 0/3] [Batch 150/216] [D loss: 0.232257] [G loss: 12.121037] time: 0:01:29.468029\n",
      "(50, 128, 128, 3)\n",
      "0.75756145\n",
      "[Epoch 0/3] [Batch 151/216] [D loss: 0.222572] [G loss: 12.555589] time: 0:01:30.054820\n",
      "(50, 128, 128, 3)\n",
      "0.7887299\n",
      "[Epoch 0/3] [Batch 152/216] [D loss: 0.225751] [G loss: 12.128443] time: 0:01:30.634826\n",
      "(50, 128, 128, 3)\n",
      "0.76857066\n",
      "[Epoch 0/3] [Batch 153/216] [D loss: 0.225986] [G loss: 13.267154] time: 0:01:31.215528\n",
      "(50, 128, 128, 3)\n",
      "0.83226323\n",
      "[Epoch 0/3] [Batch 154/216] [D loss: 0.240898] [G loss: 13.531747] time: 0:01:31.800390\n",
      "(50, 128, 128, 3)\n",
      "0.7916734\n",
      "[Epoch 0/3] [Batch 155/216] [D loss: 0.238377] [G loss: 12.658009] time: 0:01:32.390487\n",
      "(50, 128, 128, 3)\n",
      "0.82558995\n",
      "[Epoch 0/3] [Batch 156/216] [D loss: 0.243382] [G loss: 14.091342] time: 0:01:32.979295\n",
      "(50, 128, 128, 3)\n",
      "0.77498597\n",
      "[Epoch 0/3] [Batch 157/216] [D loss: 0.233231] [G loss: 12.250352] time: 0:01:33.559201\n",
      "(50, 128, 128, 3)\n",
      "0.7526266\n",
      "[Epoch 0/3] [Batch 158/216] [D loss: 0.238474] [G loss: 12.830494] time: 0:01:34.154846\n",
      "(50, 128, 128, 3)\n",
      "0.83610195\n",
      "[Epoch 0/3] [Batch 159/216] [D loss: 0.231997] [G loss: 11.770687] time: 0:01:34.739684\n",
      "(50, 128, 128, 3)\n",
      "0.7627058\n",
      "[Epoch 0/3] [Batch 160/216] [D loss: 0.231876] [G loss: 12.943714] time: 0:01:35.309941\n",
      "(50, 128, 128, 3)\n",
      "0.773942\n",
      "[Epoch 0/3] [Batch 161/216] [D loss: 0.236665] [G loss: 12.580205] time: 0:01:35.898096\n",
      "(50, 128, 128, 3)\n",
      "0.8023403\n",
      "[Epoch 0/3] [Batch 162/216] [D loss: 0.237301] [G loss: 13.064920] time: 0:01:36.487009\n",
      "(50, 128, 128, 3)\n",
      "0.81639886\n",
      "[Epoch 0/3] [Batch 163/216] [D loss: 0.242376] [G loss: 12.922852] time: 0:01:37.075942\n",
      "(50, 128, 128, 3)\n",
      "0.7270205\n",
      "[Epoch 0/3] [Batch 164/216] [D loss: 0.230660] [G loss: 11.942829] time: 0:01:37.657566\n",
      "(50, 128, 128, 3)\n",
      "0.7723406\n",
      "[Epoch 0/3] [Batch 165/216] [D loss: 0.237062] [G loss: 11.411919] time: 0:01:38.245833\n",
      "(50, 128, 128, 3)\n",
      "0.7904323\n",
      "[Epoch 0/3] [Batch 166/216] [D loss: 0.241799] [G loss: 12.635983] time: 0:01:38.823966\n",
      "(50, 128, 128, 3)\n",
      "0.83006173\n",
      "[Epoch 0/3] [Batch 167/216] [D loss: 0.238738] [G loss: 13.012568] time: 0:01:39.407606\n",
      "(50, 128, 128, 3)\n",
      "0.80717355\n",
      "[Epoch 0/3] [Batch 168/216] [D loss: 0.252702] [G loss: 13.402036] time: 0:01:39.997128\n",
      "(50, 128, 128, 3)\n",
      "0.8286198\n",
      "[Epoch 0/3] [Batch 169/216] [D loss: 0.260676] [G loss: 12.831308] time: 0:01:40.574126\n",
      "(50, 128, 128, 3)\n",
      "0.8650439\n",
      "[Epoch 0/3] [Batch 170/216] [D loss: 0.257502] [G loss: 13.193679] time: 0:01:41.187145\n",
      "(50, 128, 128, 3)\n",
      "0.85073036\n",
      "[Epoch 0/3] [Batch 171/216] [D loss: 0.244157] [G loss: 11.715598] time: 0:01:41.780887\n",
      "(50, 128, 128, 3)\n",
      "0.7716894\n",
      "[Epoch 0/3] [Batch 172/216] [D loss: 0.243414] [G loss: 12.135599] time: 0:01:42.366982\n",
      "(50, 128, 128, 3)\n",
      "0.8341792\n",
      "[Epoch 0/3] [Batch 173/216] [D loss: 0.245013] [G loss: 12.824368] time: 0:01:42.970002\n",
      "(50, 128, 128, 3)\n",
      "0.8311812\n",
      "[Epoch 0/3] [Batch 174/216] [D loss: 0.252522] [G loss: 13.424441] time: 0:01:43.556108\n",
      "(50, 128, 128, 3)\n",
      "0.78650683\n",
      "[Epoch 0/3] [Batch 175/216] [D loss: 0.251385] [G loss: 11.644094] time: 0:01:44.156049\n",
      "(50, 128, 128, 3)\n",
      "0.82355374\n",
      "[Epoch 0/3] [Batch 176/216] [D loss: 0.234640] [G loss: 14.366369] time: 0:01:44.779109\n",
      "(50, 128, 128, 3)\n",
      "0.7680326\n",
      "[Epoch 0/3] [Batch 177/216] [D loss: 0.243828] [G loss: 13.337276] time: 0:01:45.384541\n",
      "(50, 128, 128, 3)\n",
      "0.82745034\n",
      "[Epoch 0/3] [Batch 178/216] [D loss: 0.261435] [G loss: 12.158963] time: 0:01:45.969338\n",
      "(50, 128, 128, 3)\n",
      "0.84979415\n",
      "[Epoch 0/3] [Batch 179/216] [D loss: 0.247402] [G loss: 12.831740] time: 0:01:46.559071\n",
      "(50, 128, 128, 3)\n",
      "0.829035\n",
      "[Epoch 0/3] [Batch 180/216] [D loss: 0.224954] [G loss: 12.869238] time: 0:01:47.151809\n",
      "(50, 128, 128, 3)\n",
      "0.788847\n",
      "[Epoch 0/3] [Batch 181/216] [D loss: 0.236409] [G loss: 13.430331] time: 0:01:47.759499\n",
      "(50, 128, 128, 3)\n",
      "0.8579132\n",
      "[Epoch 0/3] [Batch 182/216] [D loss: 0.247121] [G loss: 15.132430] time: 0:01:48.359880\n",
      "(50, 128, 128, 3)\n",
      "0.87860036\n",
      "[Epoch 0/3] [Batch 183/216] [D loss: 0.260294] [G loss: 11.915589] time: 0:01:48.945824\n",
      "(50, 128, 128, 3)\n",
      "0.819951\n",
      "[Epoch 0/3] [Batch 184/216] [D loss: 0.254773] [G loss: 11.642821] time: 0:01:49.525797\n",
      "(50, 128, 128, 3)\n",
      "0.803549\n",
      "[Epoch 0/3] [Batch 185/216] [D loss: 0.259099] [G loss: 12.254322] time: 0:01:50.102728\n",
      "(50, 128, 128, 3)\n",
      "0.7789405\n",
      "[Epoch 0/3] [Batch 186/216] [D loss: 0.194717] [G loss: 11.929872] time: 0:01:50.690758\n",
      "(50, 128, 128, 3)\n",
      "0.6606782\n",
      "[Epoch 0/3] [Batch 187/216] [D loss: 0.148983] [G loss: 10.510206] time: 0:01:51.296547\n",
      "(50, 128, 128, 3)\n",
      "0.4077159\n",
      "[Epoch 0/3] [Batch 188/216] [D loss: 0.100117] [G loss: 10.706998] time: 0:01:51.875241\n",
      "(50, 128, 128, 3)\n",
      "0.7087479\n",
      "[Epoch 0/3] [Batch 189/216] [D loss: 0.138415] [G loss: 10.787646] time: 0:01:52.460929\n",
      "(50, 128, 128, 3)\n",
      "0.825861\n",
      "[Epoch 0/3] [Batch 190/216] [D loss: 0.234458] [G loss: 14.102074] time: 0:01:53.050939\n",
      "(50, 128, 128, 3)\n",
      "0.80897826\n",
      "[Epoch 0/3] [Batch 191/216] [D loss: 0.263885] [G loss: 9.673761] time: 0:01:53.655820\n",
      "(50, 128, 128, 3)\n",
      "0.85121113\n",
      "[Epoch 0/3] [Batch 192/216] [D loss: 0.267963] [G loss: 11.064984] time: 0:01:54.253611\n",
      "(50, 128, 128, 3)\n",
      "0.86882347\n",
      "[Epoch 0/3] [Batch 193/216] [D loss: 0.262014] [G loss: 9.648009] time: 0:01:54.846113\n",
      "(50, 128, 128, 3)\n",
      "0.83654267\n",
      "[Epoch 0/3] [Batch 194/216] [D loss: 0.265031] [G loss: 12.280634] time: 0:01:55.442722\n",
      "(50, 128, 128, 3)\n",
      "0.8886776\n",
      "[Epoch 0/3] [Batch 195/216] [D loss: 0.262110] [G loss: 10.350769] time: 0:01:56.032884\n",
      "(50, 128, 128, 3)\n",
      "0.8349502\n",
      "[Epoch 0/3] [Batch 196/216] [D loss: 0.257298] [G loss: 9.580340] time: 0:01:56.633283\n",
      "(50, 128, 128, 3)\n",
      "0.79539967\n",
      "[Epoch 0/3] [Batch 197/216] [D loss: 0.264896] [G loss: 10.175080] time: 0:01:57.215103\n",
      "(50, 128, 128, 3)\n",
      "0.8609471\n",
      "[Epoch 0/3] [Batch 198/216] [D loss: 0.260351] [G loss: 11.578628] time: 0:01:57.816317\n",
      "(50, 128, 128, 3)\n",
      "0.87697476\n",
      "[Epoch 0/3] [Batch 199/216] [D loss: 0.261032] [G loss: 9.997119] time: 0:01:58.395151\n",
      "(50, 128, 128, 3)\n",
      "0.7852573\n",
      "[Epoch 0/3] [Batch 200/216] [D loss: 0.255401] [G loss: 9.983210] time: 0:01:58.994162\n",
      "(50, 128, 128, 3)\n",
      "0.8190667\n",
      "[Epoch 0/3] [Batch 201/216] [D loss: 0.253422] [G loss: 10.373527] time: 0:01:59.594595\n",
      "(50, 128, 128, 3)\n",
      "0.81081194\n",
      "[Epoch 0/3] [Batch 202/216] [D loss: 0.258249] [G loss: 9.546900] time: 0:02:00.210217\n",
      "(50, 128, 128, 3)\n",
      "0.8432414\n",
      "[Epoch 0/3] [Batch 203/216] [D loss: 0.246964] [G loss: 8.979034] time: 0:02:00.817862\n",
      "(50, 128, 128, 3)\n",
      "0.73054415\n",
      "[Epoch 0/3] [Batch 204/216] [D loss: 0.237114] [G loss: 10.434278] time: 0:02:01.402127\n",
      "(50, 128, 128, 3)\n",
      "0.77556396\n",
      "[Epoch 0/3] [Batch 205/216] [D loss: 0.264343] [G loss: 10.079634] time: 0:02:02.026857\n",
      "(50, 128, 128, 3)\n",
      "0.73036486\n",
      "[Epoch 0/3] [Batch 206/216] [D loss: 0.244743] [G loss: 11.033675] time: 0:02:02.637652\n",
      "(50, 128, 128, 3)\n",
      "0.7655451\n",
      "[Epoch 0/3] [Batch 207/216] [D loss: 0.249033] [G loss: 9.522330] time: 0:02:03.233582\n",
      "(50, 128, 128, 3)\n",
      "0.77631885\n",
      "[Epoch 0/3] [Batch 208/216] [D loss: 0.253833] [G loss: 9.051963] time: 0:02:03.813944\n",
      "(50, 128, 128, 3)\n",
      "0.7889082\n",
      "[Epoch 0/3] [Batch 209/216] [D loss: 0.259860] [G loss: 11.159995] time: 0:02:04.389755\n",
      "(50, 128, 128, 3)\n",
      "0.8181622\n",
      "[Epoch 0/3] [Batch 210/216] [D loss: 0.263551] [G loss: 10.362397] time: 0:02:04.986031\n",
      "(50, 128, 128, 3)\n",
      "0.7941737\n",
      "[Epoch 0/3] [Batch 211/216] [D loss: 0.256702] [G loss: 9.226894] time: 0:02:05.562275\n",
      "(50, 128, 128, 3)\n",
      "0.75106126\n",
      "[Epoch 0/3] [Batch 212/216] [D loss: 0.238332] [G loss: 11.012316] time: 0:02:06.159306\n",
      "(50, 128, 128, 3)\n",
      "0.8012371\n",
      "[Epoch 0/3] [Batch 213/216] [D loss: 0.251358] [G loss: 10.063015] time: 0:02:06.760989\n",
      "(50, 128, 128, 3)\n",
      "0.8164932\n",
      "[Epoch 0/3] [Batch 214/216] [D loss: 0.259447] [G loss: 10.829406] time: 0:02:07.366883\n",
      "(50, 128, 128, 3)\n",
      "0.89689296\n",
      "[Epoch 0/3] [Batch 215/216] [D loss: 0.272211] [G loss: 11.362517] time: 0:02:07.954815\n",
      "(50, 128, 128, 3)\n",
      "0.8649809\n",
      "[Epoch 1/3] [Batch 0/216] [D loss: 0.256960] [G loss: 10.048644] time: 0:02:08.566472\n",
      "(50, 128, 128, 3)\n",
      "0.87716573\n",
      "[Epoch 1/3] [Batch 1/216] [D loss: 0.261562] [G loss: 10.854717] time: 0:02:09.172354\n",
      "(50, 128, 128, 3)\n",
      "0.759607\n",
      "[Epoch 1/3] [Batch 2/216] [D loss: 0.243437] [G loss: 10.103883] time: 0:02:09.768540\n",
      "(50, 128, 128, 3)\n",
      "0.8183009\n",
      "[Epoch 1/3] [Batch 3/216] [D loss: 0.237952] [G loss: 10.720275] time: 0:02:10.352998\n",
      "(50, 128, 128, 3)\n",
      "0.80283666\n",
      "[Epoch 1/3] [Batch 4/216] [D loss: 0.254283] [G loss: 9.486930] time: 0:02:10.940874\n",
      "(50, 128, 128, 3)\n",
      "0.8189287\n",
      "[Epoch 1/3] [Batch 5/216] [D loss: 0.241147] [G loss: 9.616247] time: 0:02:11.577093\n",
      "(50, 128, 128, 3)\n",
      "0.7862725\n",
      "[Epoch 1/3] [Batch 6/216] [D loss: 0.245189] [G loss: 10.323048] time: 0:02:12.170319\n",
      "(50, 128, 128, 3)\n",
      "0.78293306\n",
      "[Epoch 1/3] [Batch 7/216] [D loss: 0.242929] [G loss: 10.416232] time: 0:02:12.754350\n",
      "(50, 128, 128, 3)\n",
      "0.8118217\n",
      "[Epoch 1/3] [Batch 8/216] [D loss: 0.248241] [G loss: 9.114025] time: 0:02:13.356495\n",
      "(50, 128, 128, 3)\n",
      "0.8030272\n",
      "[Epoch 1/3] [Batch 9/216] [D loss: 0.242289] [G loss: 11.616031] time: 0:02:13.965236\n",
      "(50, 128, 128, 3)\n",
      "0.8567474\n",
      "[Epoch 1/3] [Batch 10/216] [D loss: 0.267118] [G loss: 11.534457] time: 0:02:14.559055\n",
      "(50, 128, 128, 3)\n",
      "0.9046683\n",
      "[Epoch 1/3] [Batch 11/216] [D loss: 0.263451] [G loss: 10.529574] time: 0:02:15.150915\n",
      "(50, 128, 128, 3)\n",
      "0.8079329\n",
      "[Epoch 1/3] [Batch 12/216] [D loss: 0.266873] [G loss: 10.619754] time: 0:02:15.778809\n",
      "(50, 128, 128, 3)\n",
      "0.8792942\n",
      "[Epoch 1/3] [Batch 13/216] [D loss: 0.254477] [G loss: 11.016009] time: 0:02:16.362442\n",
      "(50, 128, 128, 3)\n",
      "0.8421224\n",
      "[Epoch 1/3] [Batch 14/216] [D loss: 0.264104] [G loss: 10.312128] time: 0:02:16.947233\n",
      "(50, 128, 128, 3)\n",
      "0.81282824\n",
      "[Epoch 1/3] [Batch 15/216] [D loss: 0.252073] [G loss: 10.761068] time: 0:02:17.551682\n",
      "(50, 128, 128, 3)\n",
      "0.8475139\n",
      "[Epoch 1/3] [Batch 16/216] [D loss: 0.260442] [G loss: 10.477229] time: 0:02:18.159760\n",
      "(50, 128, 128, 3)\n",
      "0.873147\n",
      "[Epoch 1/3] [Batch 17/216] [D loss: 0.247231] [G loss: 9.946113] time: 0:02:18.734536\n",
      "(50, 128, 128, 3)\n",
      "0.8102224\n",
      "[Epoch 1/3] [Batch 18/216] [D loss: 0.261301] [G loss: 9.864298] time: 0:02:19.333983\n",
      "(50, 128, 128, 3)\n",
      "0.8504335\n",
      "[Epoch 1/3] [Batch 19/216] [D loss: 0.247891] [G loss: 10.163022] time: 0:02:19.926413\n",
      "(50, 128, 128, 3)\n",
      "0.8252313\n",
      "[Epoch 1/3] [Batch 20/216] [D loss: 0.251750] [G loss: 11.554082] time: 0:02:20.520541\n",
      "(50, 128, 128, 3)\n",
      "0.8539173\n",
      "[Epoch 1/3] [Batch 21/216] [D loss: 0.273160] [G loss: 10.152633] time: 0:02:21.129996\n",
      "(50, 128, 128, 3)\n",
      "0.8835287\n",
      "[Epoch 1/3] [Batch 22/216] [D loss: 0.253003] [G loss: 9.383011] time: 0:02:21.725929\n",
      "(50, 128, 128, 3)\n",
      "0.80379087\n",
      "[Epoch 1/3] [Batch 23/216] [D loss: 0.247232] [G loss: 9.115294] time: 0:02:22.334656\n",
      "(50, 128, 128, 3)\n",
      "0.78061604\n",
      "[Epoch 1/3] [Batch 24/216] [D loss: 0.236873] [G loss: 9.995709] time: 0:02:22.939501\n",
      "(50, 128, 128, 3)\n",
      "0.80105644\n",
      "[Epoch 1/3] [Batch 25/216] [D loss: 0.254944] [G loss: 9.878322] time: 0:02:23.522904\n",
      "(50, 128, 128, 3)\n",
      "0.8463297\n",
      "[Epoch 1/3] [Batch 26/216] [D loss: 0.254099] [G loss: 11.873600] time: 0:02:24.127287\n",
      "(50, 128, 128, 3)\n",
      "0.85621494\n",
      "[Epoch 1/3] [Batch 27/216] [D loss: 0.273985] [G loss: 10.156181] time: 0:02:24.720075\n",
      "(50, 128, 128, 3)\n",
      "0.87022\n",
      "[Epoch 1/3] [Batch 28/216] [D loss: 0.258837] [G loss: 10.414992] time: 0:02:25.315792\n",
      "(50, 128, 128, 3)\n",
      "0.82760143\n",
      "[Epoch 1/3] [Batch 29/216] [D loss: 0.273321] [G loss: 9.023437] time: 0:02:25.917184\n",
      "(50, 128, 128, 3)\n",
      "0.83230853\n",
      "[Epoch 1/3] [Batch 30/216] [D loss: 0.272379] [G loss: 9.737106] time: 0:02:26.509204\n",
      "(50, 128, 128, 3)\n",
      "0.8876607\n",
      "[Epoch 1/3] [Batch 31/216] [D loss: 0.247404] [G loss: 11.848181] time: 0:02:27.117190\n",
      "(50, 128, 128, 3)\n",
      "0.79238504\n",
      "[Epoch 1/3] [Batch 32/216] [D loss: 0.252302] [G loss: 10.812778] time: 0:02:27.708619\n",
      "(50, 128, 128, 3)\n",
      "0.839268\n",
      "[Epoch 1/3] [Batch 33/216] [D loss: 0.271058] [G loss: 9.698995] time: 0:02:28.308822\n",
      "(50, 128, 128, 3)\n",
      "0.88131696\n",
      "[Epoch 1/3] [Batch 34/216] [D loss: 0.267017] [G loss: 8.985321] time: 0:02:28.906258\n",
      "(50, 128, 128, 3)\n",
      "0.81572384\n",
      "[Epoch 1/3] [Batch 35/216] [D loss: 0.248372] [G loss: 10.152619] time: 0:02:29.516378\n",
      "(50, 128, 128, 3)\n",
      "0.82988095\n",
      "[Epoch 1/3] [Batch 36/216] [D loss: 0.250679] [G loss: 10.128303] time: 0:02:30.116357\n",
      "(50, 128, 128, 3)\n",
      "0.8132224\n",
      "[Epoch 1/3] [Batch 37/216] [D loss: 0.262007] [G loss: 9.592654] time: 0:02:30.736836\n",
      "(50, 128, 128, 3)\n",
      "0.83740634\n",
      "[Epoch 1/3] [Batch 38/216] [D loss: 0.269053] [G loss: 9.867585] time: 0:02:31.356434\n",
      "(50, 128, 128, 3)\n",
      "0.8770304\n",
      "[Epoch 1/3] [Batch 39/216] [D loss: 0.249004] [G loss: 9.849587] time: 0:02:31.970344\n",
      "(50, 128, 128, 3)\n",
      "0.80375814\n",
      "[Epoch 1/3] [Batch 40/216] [D loss: 0.260110] [G loss: 9.758037] time: 0:02:32.568389\n",
      "(50, 128, 128, 3)\n",
      "0.8809874\n",
      "[Epoch 1/3] [Batch 41/216] [D loss: 0.241220] [G loss: 11.644986] time: 0:02:33.192033\n",
      "(50, 128, 128, 3)\n",
      "0.7781865\n",
      "[Epoch 1/3] [Batch 42/216] [D loss: 0.260763] [G loss: 10.622615] time: 0:02:33.770712\n",
      "(50, 128, 128, 3)\n",
      "0.9073894\n",
      "[Epoch 1/3] [Batch 43/216] [D loss: 0.282606] [G loss: 9.930442] time: 0:02:34.374284\n",
      "(50, 128, 128, 3)\n",
      "0.7908492\n",
      "[Epoch 1/3] [Batch 44/216] [D loss: 0.242159] [G loss: 9.284553] time: 0:02:34.962780\n",
      "(50, 128, 128, 3)\n",
      "0.824276\n",
      "[Epoch 1/3] [Batch 45/216] [D loss: 0.247961] [G loss: 8.794294] time: 0:02:35.575388\n",
      "(50, 128, 128, 3)\n",
      "0.88109225\n",
      "[Epoch 1/3] [Batch 46/216] [D loss: 0.254709] [G loss: 8.472322] time: 0:02:36.590462\n",
      "(50, 128, 128, 3)\n",
      "0.8257563\n",
      "[Epoch 1/3] [Batch 47/216] [D loss: 0.248015] [G loss: 8.837898] time: 0:02:37.167489\n",
      "(50, 128, 128, 3)\n",
      "0.88619524\n",
      "[Epoch 1/3] [Batch 48/216] [D loss: 0.262335] [G loss: 7.511502] time: 0:02:37.750532\n",
      "(50, 128, 128, 3)\n",
      "0.8482377\n",
      "[Epoch 1/3] [Batch 49/216] [D loss: 0.258540] [G loss: 7.043025] time: 0:02:38.351862\n",
      "(50, 128, 128, 3)\n",
      "0.8222553\n",
      "[Epoch 1/3] [Batch 50/216] [D loss: 0.249028] [G loss: 8.627715] time: 0:02:38.949502\n",
      "(50, 128, 128, 3)\n",
      "0.81519765\n",
      "[Epoch 1/3] [Batch 51/216] [D loss: 0.262642] [G loss: 7.332371] time: 0:02:39.522015\n",
      "(50, 128, 128, 3)\n",
      "0.8651292\n",
      "[Epoch 1/3] [Batch 52/216] [D loss: 0.263586] [G loss: 8.075021] time: 0:02:40.133192\n",
      "(50, 128, 128, 3)\n",
      "0.89881355\n",
      "[Epoch 1/3] [Batch 53/216] [D loss: 0.265190] [G loss: 9.079534] time: 0:02:40.723803\n",
      "(50, 128, 128, 3)\n",
      "0.86151814\n",
      "[Epoch 1/3] [Batch 54/216] [D loss: 0.256886] [G loss: 8.116000] time: 0:02:41.324795\n",
      "(50, 128, 128, 3)\n",
      "0.84727997\n",
      "[Epoch 1/3] [Batch 55/216] [D loss: 0.258648] [G loss: 10.302608] time: 0:02:41.936451\n",
      "(50, 128, 128, 3)\n",
      "0.928744\n",
      "[Epoch 1/3] [Batch 56/216] [D loss: 0.272644] [G loss: 9.715616] time: 0:02:42.523860\n",
      "(50, 128, 128, 3)\n",
      "0.88100547\n",
      "[Epoch 1/3] [Batch 57/216] [D loss: 0.267024] [G loss: 7.752261] time: 0:02:43.116414\n",
      "(50, 128, 128, 3)\n",
      "0.84263784\n",
      "[Epoch 1/3] [Batch 58/216] [D loss: 0.258132] [G loss: 8.975693] time: 0:02:43.707137\n",
      "(50, 128, 128, 3)\n",
      "0.84494287\n",
      "[Epoch 1/3] [Batch 59/216] [D loss: 0.259080] [G loss: 8.971794] time: 0:02:44.307040\n",
      "(50, 128, 128, 3)\n",
      "0.8654871\n",
      "[Epoch 1/3] [Batch 61/216] [D loss: 0.273200] [G loss: 9.806933] time: 0:02:44.888481\n",
      "(50, 128, 128, 3)\n",
      "0.85114163\n",
      "[Epoch 1/3] [Batch 62/216] [D loss: 0.272897] [G loss: 10.237913] time: 0:02:45.485081\n",
      "(50, 128, 128, 3)\n",
      "0.868872\n",
      "[Epoch 1/3] [Batch 63/216] [D loss: 0.275637] [G loss: 9.529801] time: 0:02:46.061603\n",
      "(50, 128, 128, 3)\n",
      "0.8399539\n",
      "[Epoch 1/3] [Batch 64/216] [D loss: 0.283877] [G loss: 8.806198] time: 0:02:46.646159\n",
      "(50, 128, 128, 3)\n",
      "0.85782045\n",
      "[Epoch 1/3] [Batch 65/216] [D loss: 0.272442] [G loss: 9.284066] time: 0:02:47.230956\n",
      "(50, 128, 128, 3)\n",
      "0.8502299\n",
      "[Epoch 1/3] [Batch 66/216] [D loss: 0.262401] [G loss: 9.028921] time: 0:02:47.808551\n",
      "(50, 128, 128, 3)\n",
      "0.84062344\n",
      "[Epoch 1/3] [Batch 67/216] [D loss: 0.268200] [G loss: 8.894876] time: 0:02:48.400213\n",
      "(50, 128, 128, 3)\n",
      "0.8523182\n",
      "[Epoch 1/3] [Batch 68/216] [D loss: 0.254533] [G loss: 8.530718] time: 0:02:48.993308\n",
      "(50, 128, 128, 3)\n",
      "0.8155408\n",
      "[Epoch 1/3] [Batch 69/216] [D loss: 0.241456] [G loss: 10.549170] time: 0:02:49.582269\n",
      "(50, 128, 128, 3)\n",
      "0.78004915\n",
      "[Epoch 1/3] [Batch 70/216] [D loss: 0.258481] [G loss: 8.717213] time: 0:02:50.168706\n",
      "(50, 128, 128, 3)\n",
      "0.8804496\n",
      "[Epoch 1/3] [Batch 71/216] [D loss: 0.254730] [G loss: 9.199553] time: 0:02:50.757339\n",
      "(50, 128, 128, 3)\n",
      "0.8232276\n",
      "[Epoch 1/3] [Batch 72/216] [D loss: 0.258786] [G loss: 9.189287] time: 0:02:51.345355\n",
      "(50, 128, 128, 3)\n",
      "0.82321334\n",
      "[Epoch 1/3] [Batch 73/216] [D loss: 0.254953] [G loss: 9.431419] time: 0:02:51.946531\n",
      "(50, 128, 128, 3)\n",
      "0.87037295\n",
      "[Epoch 1/3] [Batch 74/216] [D loss: 0.258475] [G loss: 8.989715] time: 0:02:52.534011\n",
      "(50, 128, 128, 3)\n",
      "0.87223905\n",
      "[Epoch 1/3] [Batch 75/216] [D loss: 0.259324] [G loss: 8.810882] time: 0:02:53.122919\n",
      "(50, 128, 128, 3)\n",
      "0.80595535\n",
      "[Epoch 1/3] [Batch 76/216] [D loss: 0.251352] [G loss: 7.844607] time: 0:02:53.709076\n",
      "(50, 128, 128, 3)\n",
      "0.82374173\n",
      "[Epoch 1/3] [Batch 77/216] [D loss: 0.265056] [G loss: 8.750869] time: 0:02:54.296134\n",
      "(50, 128, 128, 3)\n",
      "0.8135662\n",
      "[Epoch 1/3] [Batch 78/216] [D loss: 0.250900] [G loss: 8.947941] time: 0:02:54.896250\n",
      "(50, 128, 128, 3)\n",
      "0.86594456\n",
      "[Epoch 1/3] [Batch 79/216] [D loss: 0.267369] [G loss: 8.111948] time: 0:02:55.498221\n",
      "(50, 128, 128, 3)\n",
      "0.84130025\n",
      "[Epoch 1/3] [Batch 80/216] [D loss: 0.253464] [G loss: 8.255315] time: 0:02:56.078174\n",
      "(50, 128, 128, 3)\n",
      "0.8697489\n",
      "[Epoch 1/3] [Batch 81/216] [D loss: 0.255383] [G loss: 8.311618] time: 0:02:56.673085\n",
      "(50, 128, 128, 3)\n",
      "0.86603254\n",
      "[Epoch 1/3] [Batch 82/216] [D loss: 0.258121] [G loss: 8.376325] time: 0:02:57.256132\n",
      "(50, 128, 128, 3)\n",
      "0.8283178\n",
      "[Epoch 1/3] [Batch 83/216] [D loss: 0.239491] [G loss: 8.890457] time: 0:02:57.849809\n",
      "(50, 128, 128, 3)\n",
      "0.82816267\n",
      "[Epoch 1/3] [Batch 84/216] [D loss: 0.238989] [G loss: 8.960052] time: 0:02:58.423580\n",
      "(50, 128, 128, 3)\n",
      "0.84711456\n",
      "[Epoch 1/3] [Batch 85/216] [D loss: 0.270272] [G loss: 9.374925] time: 0:02:59.025592\n",
      "(50, 128, 128, 3)\n",
      "0.8755789\n",
      "[Epoch 1/3] [Batch 86/216] [D loss: 0.266736] [G loss: 8.507273] time: 0:02:59.603833\n",
      "(50, 128, 128, 3)\n",
      "0.84226894\n",
      "[Epoch 1/3] [Batch 87/216] [D loss: 0.246181] [G loss: 7.808072] time: 0:03:00.182980\n",
      "(50, 128, 128, 3)\n",
      "0.83296055\n",
      "[Epoch 1/3] [Batch 88/216] [D loss: 0.257269] [G loss: 9.409901] time: 0:03:00.751346\n",
      "(50, 128, 128, 3)\n",
      "0.93073803\n",
      "[Epoch 1/3] [Batch 89/216] [D loss: 0.251573] [G loss: 9.084070] time: 0:03:01.338669\n",
      "(50, 128, 128, 3)\n",
      "0.83441734\n",
      "[Epoch 1/3] [Batch 90/216] [D loss: 0.258166] [G loss: 9.742715] time: 0:03:01.933697\n",
      "(50, 128, 128, 3)\n",
      "0.82893896\n",
      "[Epoch 1/3] [Batch 91/216] [D loss: 0.266206] [G loss: 8.630367] time: 0:03:02.536788\n",
      "(50, 128, 128, 3)\n",
      "0.8766783\n",
      "[Epoch 1/3] [Batch 92/216] [D loss: 0.250391] [G loss: 8.322751] time: 0:03:03.128164\n",
      "(50, 128, 128, 3)\n",
      "0.81324226\n",
      "[Epoch 1/3] [Batch 93/216] [D loss: 0.240406] [G loss: 9.239017] time: 0:03:03.714293\n",
      "(50, 128, 128, 3)\n",
      "0.8090325\n",
      "[Epoch 1/3] [Batch 94/216] [D loss: 0.258183] [G loss: 7.783657] time: 0:03:04.320121\n",
      "(50, 128, 128, 3)\n",
      "0.8456703\n",
      "[Epoch 1/3] [Batch 95/216] [D loss: 0.273918] [G loss: 7.820512] time: 0:03:04.926596\n",
      "(50, 128, 128, 3)\n",
      "0.886492\n",
      "[Epoch 1/3] [Batch 96/216] [D loss: 0.257014] [G loss: 11.566730] time: 0:03:05.520802\n",
      "(50, 128, 128, 3)\n",
      "0.87436956\n",
      "[Epoch 1/3] [Batch 97/216] [D loss: 0.281254] [G loss: 9.062020] time: 0:03:06.113740\n",
      "(50, 128, 128, 3)\n",
      "0.8986009\n",
      "[Epoch 1/3] [Batch 98/216] [D loss: 0.275945] [G loss: 8.305895] time: 0:03:06.702097\n",
      "(50, 128, 128, 3)\n",
      "0.7605075\n",
      "[Epoch 1/3] [Batch 99/216] [D loss: 0.251715] [G loss: 8.878168] time: 0:03:07.286577\n",
      "(50, 128, 128, 3)\n",
      "0.88400126\n",
      "[Epoch 1/3] [Batch 100/216] [D loss: 0.262202] [G loss: 8.692848] time: 0:03:07.872879\n",
      "(50, 128, 128, 3)\n",
      "0.84603864\n",
      "[Epoch 1/3] [Batch 101/216] [D loss: 0.265422] [G loss: 7.645762] time: 0:03:08.451687\n",
      "(50, 128, 128, 3)\n",
      "0.8665108\n",
      "[Epoch 1/3] [Batch 102/216] [D loss: 0.259112] [G loss: 7.767237] time: 0:03:09.038328\n",
      "(50, 128, 128, 3)\n",
      "0.82898813\n",
      "[Epoch 1/3] [Batch 103/216] [D loss: 0.258192] [G loss: 8.881894] time: 0:03:09.626172\n",
      "(50, 128, 128, 3)\n",
      "0.8858254\n",
      "[Epoch 1/3] [Batch 104/216] [D loss: 0.274373] [G loss: 10.675797] time: 0:03:10.222790\n",
      "(50, 128, 128, 3)\n",
      "0.9303697\n",
      "[Epoch 1/3] [Batch 105/216] [D loss: 0.268607] [G loss: 8.889854] time: 0:03:10.795931\n",
      "(50, 128, 128, 3)\n",
      "0.8780485\n",
      "[Epoch 1/3] [Batch 106/216] [D loss: 0.266092] [G loss: 8.757632] time: 0:03:11.403186\n",
      "(50, 128, 128, 3)\n",
      "0.84661126\n",
      "[Epoch 1/3] [Batch 107/216] [D loss: 0.254948] [G loss: 9.135109] time: 0:03:11.994253\n",
      "(50, 128, 128, 3)\n",
      "0.86900634\n",
      "[Epoch 1/3] [Batch 108/216] [D loss: 0.256162] [G loss: 9.099006] time: 0:03:12.577594\n",
      "(50, 128, 128, 3)\n",
      "0.8468862\n",
      "[Epoch 1/3] [Batch 109/216] [D loss: 0.270808] [G loss: 8.018257] time: 0:03:13.165510\n",
      "(50, 128, 128, 3)\n",
      "0.8628047\n",
      "[Epoch 1/3] [Batch 110/216] [D loss: 0.265346] [G loss: 6.948728] time: 0:03:13.755318\n",
      "(50, 128, 128, 3)\n",
      "0.8495829\n",
      "[Epoch 1/3] [Batch 111/216] [D loss: 0.262514] [G loss: 7.874215] time: 0:03:14.340937\n",
      "(50, 128, 128, 3)\n",
      "0.87988466\n",
      "[Epoch 1/3] [Batch 112/216] [D loss: 0.268393] [G loss: 8.138191] time: 0:03:14.923970\n",
      "(50, 128, 128, 3)\n",
      "0.8916139\n",
      "[Epoch 1/3] [Batch 113/216] [D loss: 0.266512] [G loss: 9.614990] time: 0:03:15.527494\n",
      "(50, 128, 128, 3)\n",
      "0.92624515\n",
      "[Epoch 1/3] [Batch 114/216] [D loss: 0.257014] [G loss: 8.907737] time: 0:03:16.108255\n",
      "(50, 128, 128, 3)\n",
      "0.89336807\n",
      "[Epoch 1/3] [Batch 115/216] [D loss: 0.256807] [G loss: 6.604636] time: 0:03:16.708808\n",
      "(50, 128, 128, 3)\n",
      "0.816832\n",
      "[Epoch 1/3] [Batch 116/216] [D loss: 0.255536] [G loss: 7.027061] time: 0:03:17.312704\n",
      "(50, 128, 128, 3)\n",
      "0.8866789\n",
      "[Epoch 1/3] [Batch 117/216] [D loss: 0.268402] [G loss: 7.214719] time: 0:03:17.894971\n",
      "(50, 128, 128, 3)\n",
      "0.8828862\n",
      "[Epoch 1/3] [Batch 118/216] [D loss: 0.259332] [G loss: 6.759727] time: 0:03:18.478705\n",
      "(50, 128, 128, 3)\n",
      "0.8127503\n",
      "[Epoch 1/3] [Batch 119/216] [D loss: 0.263508] [G loss: 6.222741] time: 0:03:19.086478\n",
      "(50, 128, 128, 3)\n",
      "0.8559591\n",
      "[Epoch 1/3] [Batch 120/216] [D loss: 0.246568] [G loss: 8.794418] time: 0:03:19.689628\n",
      "(50, 128, 128, 3)\n",
      "0.84550303\n",
      "[Epoch 1/3] [Batch 121/216] [D loss: 0.262626] [G loss: 7.203493] time: 0:03:20.274722\n",
      "(50, 128, 128, 3)\n",
      "0.854325\n",
      "[Epoch 1/3] [Batch 122/216] [D loss: 0.272241] [G loss: 6.778721] time: 0:03:20.839970\n",
      "(50, 128, 128, 3)\n",
      "0.86698484\n",
      "[Epoch 1/3] [Batch 123/216] [D loss: 0.265012] [G loss: 7.548754] time: 0:03:21.421643\n",
      "(50, 128, 128, 3)\n",
      "0.91415375\n",
      "[Epoch 1/3] [Batch 124/216] [D loss: 0.263190] [G loss: 6.661631] time: 0:03:22.012821\n",
      "(50, 128, 128, 3)\n",
      "0.8405351\n",
      "[Epoch 1/3] [Batch 125/216] [D loss: 0.261077] [G loss: 6.805418] time: 0:03:22.620708\n",
      "(50, 128, 128, 3)\n",
      "0.8078547\n",
      "[Epoch 1/3] [Batch 126/216] [D loss: 0.255321] [G loss: 6.394089] time: 0:03:23.207537\n",
      "(50, 128, 128, 3)\n",
      "0.8398312\n",
      "[Epoch 1/3] [Batch 127/216] [D loss: 0.260988] [G loss: 6.147840] time: 0:03:23.798911\n",
      "(50, 128, 128, 3)\n",
      "0.8700692\n",
      "[Epoch 1/3] [Batch 128/216] [D loss: 0.268969] [G loss: 6.114120] time: 0:03:24.396801\n",
      "(50, 128, 128, 3)\n",
      "0.8115812\n",
      "[Epoch 1/3] [Batch 129/216] [D loss: 0.266523] [G loss: 6.287801] time: 0:03:24.974885\n",
      "(50, 128, 128, 3)\n",
      "0.9165185\n",
      "[Epoch 1/3] [Batch 130/216] [D loss: 0.263380] [G loss: 6.790971] time: 0:03:25.565590\n",
      "(50, 128, 128, 3)\n",
      "0.8435369\n",
      "[Epoch 1/3] [Batch 131/216] [D loss: 0.257511] [G loss: 8.190862] time: 0:03:26.136055\n",
      "(50, 128, 128, 3)\n",
      "0.806652\n",
      "[Epoch 1/3] [Batch 132/216] [D loss: 0.259349] [G loss: 7.683013] time: 0:03:26.727598\n",
      "(50, 128, 128, 3)\n",
      "0.81932336\n",
      "[Epoch 1/3] [Batch 133/216] [D loss: 0.275261] [G loss: 6.991835] time: 0:03:27.315724\n",
      "(50, 128, 128, 3)\n",
      "0.86483866\n",
      "[Epoch 1/3] [Batch 134/216] [D loss: 0.270059] [G loss: 8.761105] time: 0:03:27.899129\n",
      "(50, 128, 128, 3)\n",
      "0.8438365\n",
      "[Epoch 1/3] [Batch 135/216] [D loss: 0.265655] [G loss: 7.874666] time: 0:03:28.484097\n",
      "(50, 128, 128, 3)\n",
      "0.8603256\n",
      "[Epoch 1/3] [Batch 136/216] [D loss: 0.268931] [G loss: 7.767036] time: 0:03:29.058317\n",
      "(50, 128, 128, 3)\n",
      "0.88405037\n",
      "[Epoch 1/3] [Batch 137/216] [D loss: 0.259478] [G loss: 7.485544] time: 0:03:29.645296\n",
      "(50, 128, 128, 3)\n",
      "0.8850915\n",
      "[Epoch 1/3] [Batch 138/216] [D loss: 0.281228] [G loss: 8.801303] time: 0:03:30.238261\n",
      "(50, 128, 128, 3)\n",
      "0.9387155\n",
      "[Epoch 1/3] [Batch 139/216] [D loss: 0.277824] [G loss: 7.339646] time: 0:03:30.825189\n",
      "(50, 128, 128, 3)\n",
      "0.8666677\n",
      "[Epoch 1/3] [Batch 140/216] [D loss: 0.268312] [G loss: 7.211048] time: 0:03:31.396810\n",
      "(50, 128, 128, 3)\n",
      "0.8605604\n",
      "[Epoch 1/3] [Batch 141/216] [D loss: 0.267918] [G loss: 7.284638] time: 0:03:31.976097\n",
      "(50, 128, 128, 3)\n",
      "0.8949624\n",
      "[Epoch 1/3] [Batch 142/216] [D loss: 0.257044] [G loss: 8.185234] time: 0:03:32.547129\n",
      "(50, 128, 128, 3)\n",
      "0.8721888\n",
      "[Epoch 1/3] [Batch 143/216] [D loss: 0.264400] [G loss: 7.291794] time: 0:03:33.138207\n",
      "(50, 128, 128, 3)\n",
      "0.91470546\n",
      "[Epoch 1/3] [Batch 144/216] [D loss: 0.274456] [G loss: 6.951976] time: 0:03:33.720721\n",
      "(50, 128, 128, 3)\n",
      "0.87314695\n",
      "[Epoch 1/3] [Batch 145/216] [D loss: 0.260278] [G loss: 6.723013] time: 0:03:34.308046\n",
      "(50, 128, 128, 3)\n",
      "0.8584756\n",
      "[Epoch 1/3] [Batch 146/216] [D loss: 0.263098] [G loss: 8.518674] time: 0:03:34.909535\n",
      "(50, 128, 128, 3)\n",
      "0.89565676\n",
      "[Epoch 1/3] [Batch 147/216] [D loss: 0.260937] [G loss: 7.588898] time: 0:03:35.495028\n",
      "(50, 128, 128, 3)\n",
      "0.8191235\n",
      "[Epoch 1/3] [Batch 148/216] [D loss: 0.263541] [G loss: 6.333574] time: 0:03:36.091615\n",
      "(50, 128, 128, 3)\n",
      "0.87525314\n",
      "[Epoch 1/3] [Batch 149/216] [D loss: 0.255943] [G loss: 7.445381] time: 0:03:36.685057\n",
      "(50, 128, 128, 3)\n",
      "0.8176937\n",
      "[Epoch 1/3] [Batch 150/216] [D loss: 0.247329] [G loss: 8.130180] time: 0:03:37.292539\n",
      "(50, 128, 128, 3)\n",
      "0.87769824\n",
      "[Epoch 1/3] [Batch 151/216] [D loss: 0.245872] [G loss: 7.089293] time: 0:03:37.879149\n",
      "(50, 128, 128, 3)\n",
      "0.8284157\n",
      "[Epoch 1/3] [Batch 152/216] [D loss: 0.245111] [G loss: 7.923646] time: 0:03:38.488100\n",
      "(50, 128, 128, 3)\n",
      "0.8634968\n",
      "[Epoch 1/3] [Batch 153/216] [D loss: 0.256150] [G loss: 6.744868] time: 0:03:39.082412\n",
      "(50, 128, 128, 3)\n",
      "0.8319815\n",
      "[Epoch 1/3] [Batch 154/216] [D loss: 0.245270] [G loss: 8.060527] time: 0:03:39.675384\n",
      "(50, 128, 128, 3)\n",
      "0.8916068\n",
      "[Epoch 1/3] [Batch 155/216] [D loss: 0.264724] [G loss: 7.210886] time: 0:03:40.263619\n",
      "(50, 128, 128, 3)\n",
      "0.84671634\n",
      "[Epoch 1/3] [Batch 156/216] [D loss: 0.255828] [G loss: 7.591271] time: 0:03:40.843255\n",
      "(50, 128, 128, 3)\n",
      "0.8848168\n",
      "[Epoch 1/3] [Batch 157/216] [D loss: 0.266666] [G loss: 7.690475] time: 0:03:41.451072\n",
      "(50, 128, 128, 3)\n",
      "0.8344626\n",
      "[Epoch 1/3] [Batch 158/216] [D loss: 0.250465] [G loss: 7.413630] time: 0:03:42.044315\n",
      "(50, 128, 128, 3)\n",
      "0.81479335\n",
      "[Epoch 1/3] [Batch 159/216] [D loss: 0.260215] [G loss: 7.260473] time: 0:03:42.637472\n",
      "(50, 128, 128, 3)\n",
      "0.89488536\n",
      "[Epoch 1/3] [Batch 160/216] [D loss: 0.249021] [G loss: 7.402570] time: 0:03:43.233553\n",
      "(50, 128, 128, 3)\n",
      "0.8215671\n",
      "[Epoch 1/3] [Batch 161/216] [D loss: 0.252536] [G loss: 7.972109] time: 0:03:43.839828\n",
      "(50, 128, 128, 3)\n",
      "0.8324046\n",
      "[Epoch 1/3] [Batch 162/216] [D loss: 0.255956] [G loss: 7.840571] time: 0:03:44.445891\n",
      "(50, 128, 128, 3)\n",
      "0.85968226\n",
      "[Epoch 1/3] [Batch 163/216] [D loss: 0.255901] [G loss: 7.687447] time: 0:03:45.040090\n",
      "(50, 128, 128, 3)\n",
      "0.87519854\n",
      "[Epoch 1/3] [Batch 164/216] [D loss: 0.265245] [G loss: 7.625964] time: 0:03:45.640672\n",
      "(50, 128, 128, 3)\n",
      "0.7809403\n",
      "[Epoch 1/3] [Batch 165/216] [D loss: 0.244997] [G loss: 7.920586] time: 0:03:46.235037\n",
      "(50, 128, 128, 3)\n",
      "0.82858706\n",
      "[Epoch 1/3] [Batch 166/216] [D loss: 0.254735] [G loss: 6.780083] time: 0:03:46.852050\n",
      "(50, 128, 128, 3)\n",
      "0.8475637\n",
      "[Epoch 1/3] [Batch 167/216] [D loss: 0.260832] [G loss: 7.076882] time: 0:03:47.452483\n",
      "(50, 128, 128, 3)\n",
      "0.87928987\n",
      "[Epoch 1/3] [Batch 168/216] [D loss: 0.251366] [G loss: 8.038270] time: 0:03:48.065692\n",
      "(50, 128, 128, 3)\n",
      "0.85217214\n",
      "[Epoch 1/3] [Batch 169/216] [D loss: 0.266300] [G loss: 7.584695] time: 0:03:48.654690\n",
      "(50, 128, 128, 3)\n",
      "0.8711887\n",
      "[Epoch 1/3] [Batch 170/216] [D loss: 0.275316] [G loss: 6.980987] time: 0:03:49.252311\n",
      "(50, 128, 128, 3)\n",
      "0.9116886\n",
      "[Epoch 1/3] [Batch 171/216] [D loss: 0.271116] [G loss: 7.353629] time: 0:03:49.835679\n",
      "(50, 128, 128, 3)\n",
      "0.89569855\n",
      "[Epoch 1/3] [Batch 172/216] [D loss: 0.255484] [G loss: 7.256189] time: 0:03:50.415487\n",
      "(50, 128, 128, 3)\n",
      "0.81770974\n",
      "[Epoch 1/3] [Batch 173/216] [D loss: 0.257098] [G loss: 7.193215] time: 0:03:51.005952\n",
      "(50, 128, 128, 3)\n",
      "0.8817075\n",
      "[Epoch 1/3] [Batch 174/216] [D loss: 0.262069] [G loss: 7.405043] time: 0:03:51.600447\n",
      "(50, 128, 128, 3)\n",
      "0.8861923\n",
      "[Epoch 1/3] [Batch 175/216] [D loss: 0.273998] [G loss: 8.175914] time: 0:03:52.184396\n",
      "(50, 128, 128, 3)\n",
      "0.8316024\n",
      "[Epoch 1/3] [Batch 176/216] [D loss: 0.265387] [G loss: 6.727890] time: 0:03:52.780853\n",
      "(50, 128, 128, 3)\n",
      "0.8738365\n",
      "[Epoch 1/3] [Batch 177/216] [D loss: 0.251056] [G loss: 8.814663] time: 0:03:53.368723\n",
      "(50, 128, 128, 3)\n",
      "0.8307991\n",
      "[Epoch 1/3] [Batch 178/216] [D loss: 0.266751] [G loss: 8.378600] time: 0:03:53.959120\n",
      "(50, 128, 128, 3)\n",
      "0.8790088\n",
      "[Epoch 1/3] [Batch 179/216] [D loss: 0.279862] [G loss: 6.788985] time: 0:03:54.570430\n",
      "(50, 128, 128, 3)\n",
      "0.9038295\n",
      "[Epoch 1/3] [Batch 180/216] [D loss: 0.265335] [G loss: 7.199410] time: 0:03:55.154772\n",
      "(50, 128, 128, 3)\n",
      "0.8891217\n",
      "[Epoch 1/3] [Batch 181/216] [D loss: 0.246582] [G loss: 8.046165] time: 0:03:55.758237\n",
      "(50, 128, 128, 3)\n",
      "0.8279039\n",
      "[Epoch 1/3] [Batch 182/216] [D loss: 0.239216] [G loss: 7.697603] time: 0:03:56.338935\n",
      "(50, 128, 128, 3)\n",
      "0.9079723\n",
      "[Epoch 1/3] [Batch 183/216] [D loss: 0.264833] [G loss: 9.185355] time: 0:03:56.919373\n",
      "(50, 128, 128, 3)\n",
      "0.91622704\n",
      "[Epoch 1/3] [Batch 184/216] [D loss: 0.265978] [G loss: 6.861670] time: 0:03:57.506618\n",
      "(50, 128, 128, 3)\n",
      "0.85496956\n",
      "[Epoch 1/3] [Batch 185/216] [D loss: 0.256471] [G loss: 7.016186] time: 0:03:58.087644\n",
      "(50, 128, 128, 3)\n",
      "0.8328102\n",
      "[Epoch 1/3] [Batch 186/216] [D loss: 0.255430] [G loss: 6.783864] time: 0:03:58.671477\n",
      "(50, 128, 128, 3)\n",
      "0.9448304\n",
      "[Epoch 1/3] [Batch 187/216] [D loss: 0.272963] [G loss: 6.420524] time: 0:03:59.269512\n",
      "(50, 128, 128, 3)\n",
      "0.87478155\n",
      "[Epoch 1/3] [Batch 188/216] [D loss: 0.269913] [G loss: 5.641528] time: 0:03:59.861197\n",
      "(50, 128, 128, 3)\n",
      "0.8428259\n",
      "[Epoch 1/3] [Batch 189/216] [D loss: 0.269646] [G loss: 5.320368] time: 0:04:00.459396\n",
      "(50, 128, 128, 3)\n",
      "0.8886762\n",
      "[Epoch 1/3] [Batch 190/216] [D loss: 0.262143] [G loss: 5.742068] time: 0:04:01.045500\n",
      "(50, 128, 128, 3)\n",
      "0.8879819\n",
      "[Epoch 1/3] [Batch 191/216] [D loss: 0.262489] [G loss: 8.071669] time: 0:04:01.642861\n",
      "(50, 128, 128, 3)\n",
      "0.8383298\n",
      "[Epoch 1/3] [Batch 192/216] [D loss: 0.268922] [G loss: 5.083636] time: 0:04:02.226798\n",
      "(50, 128, 128, 3)\n",
      "0.87540364\n",
      "[Epoch 1/3] [Batch 193/216] [D loss: 0.264107] [G loss: 5.953760] time: 0:04:02.824783\n",
      "(50, 128, 128, 3)\n",
      "0.89343786\n",
      "[Epoch 1/3] [Batch 194/216] [D loss: 0.259924] [G loss: 5.331852] time: 0:04:03.422678\n",
      "(50, 128, 128, 3)\n",
      "0.8716874\n",
      "[Epoch 1/3] [Batch 195/216] [D loss: 0.269435] [G loss: 6.599124] time: 0:04:04.013537\n",
      "(50, 128, 128, 3)\n",
      "0.9224021\n",
      "[Epoch 1/3] [Batch 196/216] [D loss: 0.262737] [G loss: 5.776237] time: 0:04:04.597474\n",
      "(50, 128, 128, 3)\n",
      "0.87141734\n",
      "[Epoch 1/3] [Batch 197/216] [D loss: 0.260296] [G loss: 5.677650] time: 0:04:05.179636\n",
      "(50, 128, 128, 3)\n",
      "0.83387166\n",
      "[Epoch 1/3] [Batch 198/216] [D loss: 0.271366] [G loss: 5.635252] time: 0:04:05.775300\n",
      "(50, 128, 128, 3)\n",
      "0.8991981\n",
      "[Epoch 1/3] [Batch 199/216] [D loss: 0.265830] [G loss: 6.513412] time: 0:04:06.363339\n",
      "(50, 128, 128, 3)\n",
      "0.9078803\n",
      "[Epoch 1/3] [Batch 200/216] [D loss: 0.260421] [G loss: 5.864561] time: 0:04:06.948844\n",
      "(50, 128, 128, 3)\n",
      "0.8269555\n",
      "[Epoch 1/3] [Batch 201/216] [D loss: 0.263877] [G loss: 6.014459] time: 0:04:07.538761\n",
      "(50, 128, 128, 3)\n",
      "0.8669751\n",
      "[Epoch 1/3] [Batch 202/216] [D loss: 0.268308] [G loss: 5.837252] time: 0:04:08.136557\n",
      "(50, 128, 128, 3)\n",
      "0.85452646\n",
      "[Epoch 1/3] [Batch 203/216] [D loss: 0.266665] [G loss: 4.991622] time: 0:04:08.725639\n",
      "(50, 128, 128, 3)\n",
      "0.8932012\n",
      "[Epoch 1/3] [Batch 204/216] [D loss: 0.262048] [G loss: 5.446289] time: 0:04:09.319884\n",
      "(50, 128, 128, 3)\n",
      "0.7794644\n",
      "[Epoch 1/3] [Batch 205/216] [D loss: 0.255545] [G loss: 6.901406] time: 0:04:09.933099\n",
      "(50, 128, 128, 3)\n",
      "0.5739722\n",
      "[Epoch 1/3] [Batch 206/216] [D loss: 0.165943] [G loss: 5.467008] time: 0:04:10.534097\n",
      "(50, 128, 128, 3)\n",
      "0.77859473\n",
      "[Epoch 1/3] [Batch 207/216] [D loss: 0.258154] [G loss: 7.192173] time: 0:04:11.119606\n",
      "(50, 128, 128, 3)\n",
      "0.81599635\n",
      "[Epoch 1/3] [Batch 208/216] [D loss: 0.268159] [G loss: 5.505339] time: 0:04:11.701393\n",
      "(50, 128, 128, 3)\n",
      "0.83500725\n",
      "[Epoch 1/3] [Batch 209/216] [D loss: 0.271142] [G loss: 4.542853] time: 0:04:12.302929\n",
      "(50, 128, 128, 3)\n",
      "0.8436914\n",
      "[Epoch 1/3] [Batch 210/216] [D loss: 0.262758] [G loss: 6.747453] time: 0:04:12.912052\n",
      "(50, 128, 128, 3)\n",
      "0.8506551\n",
      "[Epoch 1/3] [Batch 211/216] [D loss: 0.263739] [G loss: 6.214969] time: 0:04:13.493251\n",
      "(50, 128, 128, 3)\n",
      "0.839194\n",
      "[Epoch 1/3] [Batch 212/216] [D loss: 0.266178] [G loss: 4.643207] time: 0:04:14.076079\n",
      "(50, 128, 128, 3)\n",
      "0.78894585\n",
      "[Epoch 1/3] [Batch 213/216] [D loss: 0.241551] [G loss: 7.055265] time: 0:04:14.662979\n",
      "(50, 128, 128, 3)\n",
      "0.8366201\n",
      "[Epoch 1/3] [Batch 214/216] [D loss: 0.257677] [G loss: 5.711215] time: 0:04:15.245992\n",
      "(50, 128, 128, 3)\n",
      "0.8615803\n",
      "[Epoch 1/3] [Batch 215/216] [D loss: 0.270987] [G loss: 5.660833] time: 0:04:15.857410\n",
      "(50, 128, 128, 3)\n",
      "0.9111095\n",
      "[Epoch 2/3] [Batch 0/216] [D loss: 0.258844] [G loss: 6.290601] time: 0:04:16.467122\n",
      "(50, 128, 128, 3)\n",
      "0.89090127\n",
      "[Epoch 2/3] [Batch 1/216] [D loss: 0.258952] [G loss: 5.331580] time: 0:04:17.059331\n",
      "(50, 128, 128, 3)\n",
      "0.90114737\n",
      "[Epoch 2/3] [Batch 2/216] [D loss: 0.257002] [G loss: 6.400227] time: 0:04:17.637233\n",
      "(50, 128, 128, 3)\n",
      "0.8017723\n",
      "[Epoch 2/3] [Batch 3/216] [D loss: 0.253358] [G loss: 6.207541] time: 0:04:18.235482\n",
      "(50, 128, 128, 3)\n",
      "0.8715522\n",
      "[Epoch 2/3] [Batch 4/216] [D loss: 0.260265] [G loss: 6.294222] time: 0:04:18.845227\n",
      "(50, 128, 128, 3)\n",
      "0.8586189\n",
      "[Epoch 2/3] [Batch 5/216] [D loss: 0.273047] [G loss: 4.880307] time: 0:04:19.461453\n",
      "(50, 128, 128, 3)\n",
      "0.84920096\n",
      "[Epoch 2/3] [Batch 6/216] [D loss: 0.248929] [G loss: 5.621126] time: 0:04:20.064370\n",
      "(50, 128, 128, 3)\n",
      "0.83730006\n",
      "[Epoch 2/3] [Batch 7/216] [D loss: 0.259197] [G loss: 6.008132] time: 0:04:20.660325\n",
      "(50, 128, 128, 3)\n",
      "0.83757836\n",
      "[Epoch 2/3] [Batch 8/216] [D loss: 0.265706] [G loss: 6.022876] time: 0:04:21.245448\n",
      "(50, 128, 128, 3)\n",
      "0.8676808\n",
      "[Epoch 2/3] [Batch 9/216] [D loss: 0.267234] [G loss: 4.930132] time: 0:04:21.820376\n",
      "(50, 128, 128, 3)\n",
      "0.85629195\n",
      "[Epoch 2/3] [Batch 10/216] [D loss: 0.260652] [G loss: 6.615229] time: 0:04:22.424597\n",
      "(50, 128, 128, 3)\n",
      "0.892986\n",
      "[Epoch 2/3] [Batch 11/216] [D loss: 0.267513] [G loss: 6.736606] time: 0:04:23.010771\n",
      "(50, 128, 128, 3)\n",
      "0.92664117\n",
      "[Epoch 2/3] [Batch 12/216] [D loss: 0.263796] [G loss: 6.329545] time: 0:04:23.586485\n",
      "(50, 128, 128, 3)\n",
      "0.8434513\n",
      "[Epoch 2/3] [Batch 13/216] [D loss: 0.265744] [G loss: 5.996233] time: 0:04:24.191216\n",
      "(50, 128, 128, 3)\n",
      "0.91762286\n",
      "[Epoch 2/3] [Batch 14/216] [D loss: 0.256855] [G loss: 6.666213] time: 0:04:24.778080\n",
      "(50, 128, 128, 3)\n",
      "0.8854072\n",
      "[Epoch 2/3] [Batch 15/216] [D loss: 0.269576] [G loss: 5.716142] time: 0:04:25.358704\n",
      "(50, 128, 128, 3)\n",
      "0.8532779\n",
      "[Epoch 2/3] [Batch 16/216] [D loss: 0.259521] [G loss: 6.746833] time: 0:04:25.966069\n",
      "(50, 128, 128, 3)\n",
      "0.8838547\n",
      "[Epoch 2/3] [Batch 17/216] [D loss: 0.265005] [G loss: 6.208449] time: 0:04:26.567111\n",
      "(50, 128, 128, 3)\n",
      "0.9069144\n",
      "[Epoch 2/3] [Batch 18/216] [D loss: 0.260906] [G loss: 5.658196] time: 0:04:27.167249\n",
      "(50, 128, 128, 3)\n",
      "0.8552174\n",
      "[Epoch 2/3] [Batch 19/216] [D loss: 0.259190] [G loss: 5.782727] time: 0:04:27.779122\n",
      "(50, 128, 128, 3)\n",
      "0.8950712\n",
      "[Epoch 2/3] [Batch 20/216] [D loss: 0.260271] [G loss: 5.932784] time: 0:04:28.360735\n",
      "(50, 128, 128, 3)\n",
      "0.864002\n",
      "[Epoch 2/3] [Batch 21/216] [D loss: 0.262521] [G loss: 7.317090] time: 0:04:28.941120\n",
      "(50, 128, 128, 3)\n",
      "0.89129996\n",
      "[Epoch 2/3] [Batch 22/216] [D loss: 0.273039] [G loss: 5.631678] time: 0:04:29.526664\n",
      "(50, 128, 128, 3)\n",
      "0.913408\n",
      "[Epoch 2/3] [Batch 23/216] [D loss: 0.261969] [G loss: 5.264458] time: 0:04:30.104320\n",
      "(50, 128, 128, 3)\n",
      "0.8429337\n",
      "[Epoch 2/3] [Batch 24/216] [D loss: 0.258746] [G loss: 4.984953] time: 0:04:30.707410\n",
      "(50, 128, 128, 3)\n",
      "0.838726\n",
      "[Epoch 2/3] [Batch 25/216] [D loss: 0.260489] [G loss: 5.544169] time: 0:04:31.315496\n",
      "(50, 128, 128, 3)\n",
      "0.85590935\n",
      "[Epoch 2/3] [Batch 26/216] [D loss: 0.272028] [G loss: 5.325976] time: 0:04:31.921008\n",
      "(50, 128, 128, 3)\n",
      "0.8989267\n",
      "[Epoch 2/3] [Batch 27/216] [D loss: 0.271401] [G loss: 6.780931] time: 0:04:32.520768\n",
      "(50, 128, 128, 3)\n",
      "0.8949656\n",
      "[Epoch 2/3] [Batch 28/216] [D loss: 0.271328] [G loss: 5.560803] time: 0:04:33.122374\n",
      "(50, 128, 128, 3)\n",
      "0.9024518\n",
      "[Epoch 2/3] [Batch 29/216] [D loss: 0.263145] [G loss: 5.794053] time: 0:04:33.729484\n",
      "(50, 128, 128, 3)\n",
      "0.8607442\n",
      "[Epoch 2/3] [Batch 30/216] [D loss: 0.270694] [G loss: 4.905050] time: 0:04:34.326125\n",
      "(50, 128, 128, 3)\n",
      "0.8716739\n",
      "[Epoch 2/3] [Batch 31/216] [D loss: 0.273402] [G loss: 6.453910] time: 0:04:34.936421\n",
      "(50, 128, 128, 3)\n",
      "0.91917133\n",
      "[Epoch 2/3] [Batch 32/216] [D loss: 0.262698] [G loss: 7.329843] time: 0:04:35.527484\n",
      "(50, 128, 128, 3)\n",
      "0.8180255\n",
      "[Epoch 2/3] [Batch 33/216] [D loss: 0.247531] [G loss: 6.453176] time: 0:04:36.111241\n",
      "(50, 128, 128, 3)\n",
      "0.88978535\n",
      "[Epoch 2/3] [Batch 34/216] [D loss: 0.271897] [G loss: 5.755208] time: 0:04:36.706506\n",
      "(50, 128, 128, 3)\n",
      "0.9077334\n",
      "[Epoch 2/3] [Batch 35/216] [D loss: 0.264215] [G loss: 5.228892] time: 0:04:37.300573\n",
      "(50, 128, 128, 3)\n",
      "0.8488552\n",
      "[Epoch 2/3] [Batch 36/216] [D loss: 0.254876] [G loss: 6.051242] time: 0:04:37.883879\n",
      "(50, 128, 128, 3)\n",
      "0.8627952\n",
      "[Epoch 2/3] [Batch 37/216] [D loss: 0.259491] [G loss: 5.513391] time: 0:04:38.489691\n",
      "(50, 128, 128, 3)\n",
      "0.8403452\n",
      "[Epoch 2/3] [Batch 38/216] [D loss: 0.254347] [G loss: 5.134494] time: 0:04:39.068736\n",
      "(50, 128, 128, 3)\n",
      "0.8663697\n",
      "[Epoch 2/3] [Batch 39/216] [D loss: 0.260967] [G loss: 6.005633] time: 0:04:39.666832\n",
      "(50, 128, 128, 3)\n",
      "0.8927858\n",
      "[Epoch 2/3] [Batch 40/216] [D loss: 0.252825] [G loss: 5.490885] time: 0:04:40.253160\n",
      "(50, 128, 128, 3)\n",
      "0.82010347\n",
      "[Epoch 2/3] [Batch 41/216] [D loss: 0.241504] [G loss: 5.416984] time: 0:04:40.851416\n",
      "(50, 128, 128, 3)\n",
      "0.9086811\n",
      "[Epoch 2/3] [Batch 42/216] [D loss: 0.243630] [G loss: 7.159194] time: 0:04:41.442283\n",
      "(50, 128, 128, 3)\n",
      "0.8265295\n",
      "[Epoch 2/3] [Batch 43/216] [D loss: 0.256728] [G loss: 6.230053] time: 0:04:42.027745\n",
      "(50, 128, 128, 3)\n",
      "0.9233659\n",
      "[Epoch 2/3] [Batch 44/216] [D loss: 0.268012] [G loss: 5.944762] time: 0:04:42.609838\n",
      "(50, 128, 128, 3)\n",
      "0.84346795\n",
      "[Epoch 2/3] [Batch 45/216] [D loss: 0.262064] [G loss: 6.254448] time: 0:04:43.196703\n",
      "(50, 128, 128, 3)\n",
      "0.87056226\n",
      "[Epoch 2/3] [Batch 46/216] [D loss: 0.264116] [G loss: 5.590017] time: 0:04:43.785843\n",
      "(50, 128, 128, 3)\n",
      "0.91359764\n",
      "[Epoch 2/3] [Batch 47/216] [D loss: 0.257674] [G loss: 5.668571] time: 0:04:44.381342\n",
      "(50, 128, 128, 3)\n",
      "0.8654849\n",
      "[Epoch 2/3] [Batch 48/216] [D loss: 0.260295] [G loss: 5.585299] time: 0:04:44.974055\n",
      "(50, 128, 128, 3)\n",
      "0.91317934\n",
      "[Epoch 2/3] [Batch 49/216] [D loss: 0.261207] [G loss: 4.778019] time: 0:04:45.552443\n",
      "(50, 128, 128, 3)\n",
      "0.8770318\n",
      "[Epoch 2/3] [Batch 50/216] [D loss: 0.260794] [G loss: 4.378094] time: 0:04:46.141784\n",
      "(50, 128, 128, 3)\n",
      "0.86069316\n",
      "[Epoch 2/3] [Batch 51/216] [D loss: 0.258963] [G loss: 5.892001] time: 0:04:46.727577\n",
      "(50, 128, 128, 3)\n",
      "0.8496139\n",
      "[Epoch 2/3] [Batch 52/216] [D loss: 0.268792] [G loss: 4.452130] time: 0:04:47.317458\n",
      "(50, 128, 128, 3)\n",
      "0.8917076\n",
      "[Epoch 2/3] [Batch 53/216] [D loss: 0.261266] [G loss: 5.125048] time: 0:04:47.912419\n",
      "(50, 128, 128, 3)\n",
      "0.927686\n",
      "[Epoch 2/3] [Batch 54/216] [D loss: 0.265062] [G loss: 5.656836] time: 0:04:48.512759\n",
      "(50, 128, 128, 3)\n",
      "0.8921116\n",
      "[Epoch 2/3] [Batch 55/216] [D loss: 0.259564] [G loss: 5.389311] time: 0:04:49.115540\n",
      "(50, 128, 128, 3)\n",
      "0.8812011\n",
      "[Epoch 2/3] [Batch 56/216] [D loss: 0.263686] [G loss: 6.319945] time: 0:04:49.699391\n",
      "(50, 128, 128, 3)\n",
      "0.9471919\n",
      "[Epoch 2/3] [Batch 57/216] [D loss: 0.261244] [G loss: 6.541032] time: 0:04:50.282101\n",
      "(50, 128, 128, 3)\n",
      "0.8985569\n",
      "[Epoch 2/3] [Batch 58/216] [D loss: 0.257264] [G loss: 5.323042] time: 0:04:50.869695\n",
      "(50, 128, 128, 3)\n",
      "0.878174\n",
      "[Epoch 2/3] [Batch 59/216] [D loss: 0.263168] [G loss: 5.934379] time: 0:04:51.458580\n",
      "(50, 128, 128, 3)\n",
      "0.8765247\n",
      "[Epoch 2/3] [Batch 60/216] [D loss: 0.260835] [G loss: 6.037310] time: 0:04:52.061184\n",
      "(50, 128, 128, 3)\n",
      "0.88261396\n",
      "[Epoch 2/3] [Batch 62/216] [D loss: 0.262911] [G loss: 6.326529] time: 0:04:52.650686\n",
      "(50, 128, 128, 3)\n",
      "0.8754628\n",
      "[Epoch 2/3] [Batch 63/216] [D loss: 0.267622] [G loss: 5.949799] time: 0:04:53.234203\n",
      "(50, 128, 128, 3)\n",
      "0.89389515\n",
      "[Epoch 2/3] [Batch 64/216] [D loss: 0.265101] [G loss: 5.262838] time: 0:04:53.820247\n",
      "(50, 128, 128, 3)\n",
      "0.8497947\n",
      "[Epoch 2/3] [Batch 65/216] [D loss: 0.270536] [G loss: 5.386520] time: 0:04:54.403733\n",
      "(50, 128, 128, 3)\n",
      "0.8998925\n",
      "[Epoch 2/3] [Batch 66/216] [D loss: 0.275234] [G loss: 5.003572] time: 0:04:55.000392\n",
      "(50, 128, 128, 3)\n",
      "0.8951342\n",
      "[Epoch 2/3] [Batch 67/216] [D loss: 0.268181] [G loss: 5.062591] time: 0:04:55.586209\n",
      "(50, 128, 128, 3)\n",
      "0.88232726\n",
      "[Epoch 2/3] [Batch 68/216] [D loss: 0.274595] [G loss: 4.824371] time: 0:04:56.172815\n",
      "(50, 128, 128, 3)\n",
      "0.8893383\n",
      "[Epoch 2/3] [Batch 69/216] [D loss: 0.260201] [G loss: 4.837451] time: 0:04:56.782310\n",
      "(50, 128, 128, 3)\n",
      "0.8538208\n",
      "[Epoch 2/3] [Batch 70/216] [D loss: 0.252580] [G loss: 6.744449] time: 0:04:57.373030\n",
      "(50, 128, 128, 3)\n",
      "0.831341\n",
      "[Epoch 2/3] [Batch 71/216] [D loss: 0.269845] [G loss: 4.648529] time: 0:04:57.979427\n",
      "(50, 128, 128, 3)\n",
      "0.91553384\n",
      "[Epoch 2/3] [Batch 72/216] [D loss: 0.270454] [G loss: 4.650656] time: 0:04:58.586553\n",
      "(50, 128, 128, 3)\n",
      "0.8466885\n",
      "[Epoch 2/3] [Batch 73/216] [D loss: 0.255140] [G loss: 5.072327] time: 0:04:59.169490\n",
      "(50, 128, 128, 3)\n",
      "0.88051724\n",
      "[Epoch 2/3] [Batch 74/216] [D loss: 0.272535] [G loss: 5.181756] time: 0:04:59.751984\n",
      "(50, 128, 128, 3)\n",
      "0.9229401\n",
      "[Epoch 2/3] [Batch 75/216] [D loss: 0.278014] [G loss: 4.778343] time: 0:05:00.330227\n",
      "(50, 128, 128, 3)\n",
      "0.9187184\n",
      "[Epoch 2/3] [Batch 76/216] [D loss: 0.274189] [G loss: 4.879937] time: 0:05:00.920278\n",
      "(50, 128, 128, 3)\n",
      "0.8589089\n",
      "[Epoch 2/3] [Batch 77/216] [D loss: 0.270581] [G loss: 4.111810] time: 0:05:01.503946\n",
      "(50, 128, 128, 3)\n",
      "0.87067366\n",
      "[Epoch 2/3] [Batch 78/216] [D loss: 0.268259] [G loss: 5.342134] time: 0:05:02.087654\n",
      "(50, 128, 128, 3)\n",
      "0.85425574\n",
      "[Epoch 2/3] [Batch 79/216] [D loss: 0.259431] [G loss: 5.587875] time: 0:05:02.674518\n",
      "(50, 128, 128, 3)\n",
      "0.9025827\n",
      "[Epoch 2/3] [Batch 80/216] [D loss: 0.272739] [G loss: 4.652091] time: 0:05:03.272501\n",
      "(50, 128, 128, 3)\n",
      "0.87760067\n",
      "[Epoch 2/3] [Batch 81/216] [D loss: 0.249497] [G loss: 5.398580] time: 0:05:03.852682\n",
      "(50, 128, 128, 3)\n",
      "0.89890784\n",
      "[Epoch 2/3] [Batch 82/216] [D loss: 0.258490] [G loss: 5.097764] time: 0:05:04.445214\n",
      "(50, 128, 128, 3)\n",
      "0.903887\n",
      "[Epoch 2/3] [Batch 83/216] [D loss: 0.268233] [G loss: 4.603189] time: 0:05:05.021692\n",
      "(50, 128, 128, 3)\n",
      "0.8710584\n",
      "[Epoch 2/3] [Batch 84/216] [D loss: 0.252282] [G loss: 5.386512] time: 0:05:05.607983\n",
      "(50, 128, 128, 3)\n",
      "0.883795\n",
      "[Epoch 2/3] [Batch 85/216] [D loss: 0.261724] [G loss: 4.616021] time: 0:05:06.183198\n",
      "(50, 128, 128, 3)\n",
      "0.86162597\n",
      "[Epoch 2/3] [Batch 86/216] [D loss: 0.257614] [G loss: 5.837891] time: 0:05:06.776804\n",
      "(50, 128, 128, 3)\n",
      "0.8985958\n",
      "[Epoch 2/3] [Batch 87/216] [D loss: 0.264472] [G loss: 5.001687] time: 0:05:07.381369\n",
      "(50, 128, 128, 3)\n",
      "0.8880263\n",
      "[Epoch 2/3] [Batch 88/216] [D loss: 0.261946] [G loss: 4.275632] time: 0:05:07.963254\n",
      "(50, 128, 128, 3)\n",
      "0.87897015\n",
      "[Epoch 2/3] [Batch 89/216] [D loss: 0.262610] [G loss: 5.694582] time: 0:05:08.566710\n",
      "(50, 128, 128, 3)\n",
      "0.93839335\n",
      "[Epoch 2/3] [Batch 90/216] [D loss: 0.253587] [G loss: 5.430584] time: 0:05:09.169843\n",
      "(50, 128, 128, 3)\n",
      "0.86546016\n",
      "[Epoch 2/3] [Batch 91/216] [D loss: 0.257020] [G loss: 5.775951] time: 0:05:09.758491\n",
      "(50, 128, 128, 3)\n",
      "0.8677276\n",
      "[Epoch 2/3] [Batch 92/216] [D loss: 0.264161] [G loss: 5.034466] time: 0:05:10.347081\n",
      "(50, 128, 128, 3)\n",
      "0.9126065\n",
      "[Epoch 2/3] [Batch 93/216] [D loss: 0.260098] [G loss: 5.024674] time: 0:05:10.936335\n",
      "(50, 128, 128, 3)\n",
      "0.8526404\n",
      "[Epoch 2/3] [Batch 94/216] [D loss: 0.253293] [G loss: 5.983762] time: 0:05:11.533197\n",
      "(50, 128, 128, 3)\n",
      "0.8418131\n",
      "[Epoch 2/3] [Batch 95/216] [D loss: 0.260582] [G loss: 4.202505] time: 0:05:12.131658\n",
      "(50, 128, 128, 3)\n",
      "0.873707\n",
      "[Epoch 2/3] [Batch 96/216] [D loss: 0.263365] [G loss: 4.856012] time: 0:05:12.728820\n",
      "(50, 128, 128, 3)\n",
      "0.9183429\n",
      "[Epoch 2/3] [Batch 97/216] [D loss: 0.260877] [G loss: 7.328937] time: 0:05:13.330327\n",
      "(50, 128, 128, 3)\n",
      "0.9083788\n",
      "[Epoch 2/3] [Batch 98/216] [D loss: 0.273867] [G loss: 5.052501] time: 0:05:13.914980\n",
      "(50, 128, 128, 3)\n",
      "0.92025024\n",
      "[Epoch 2/3] [Batch 99/216] [D loss: 0.266954] [G loss: 4.979156] time: 0:05:14.507536\n",
      "(50, 128, 128, 3)\n",
      "0.80031085\n",
      "[Epoch 2/3] [Batch 100/216] [D loss: 0.259924] [G loss: 5.424231] time: 0:05:15.100527\n",
      "(50, 128, 128, 3)\n",
      "0.9169572\n",
      "[Epoch 2/3] [Batch 101/216] [D loss: 0.266157] [G loss: 5.182605] time: 0:05:15.706839\n",
      "(50, 128, 128, 3)\n",
      "0.8781648\n",
      "[Epoch 2/3] [Batch 102/216] [D loss: 0.265411] [G loss: 4.504748] time: 0:05:16.296354\n",
      "(50, 128, 128, 3)\n",
      "0.90185887\n",
      "[Epoch 2/3] [Batch 103/216] [D loss: 0.262895] [G loss: 4.806275] time: 0:05:16.892271\n",
      "(50, 128, 128, 3)\n",
      "0.8621521\n",
      "[Epoch 2/3] [Batch 104/216] [D loss: 0.262460] [G loss: 5.258735] time: 0:05:17.499860\n",
      "(50, 128, 128, 3)\n",
      "0.915098\n",
      "[Epoch 2/3] [Batch 105/216] [D loss: 0.268550] [G loss: 6.446508] time: 0:05:18.105992\n",
      "(50, 128, 128, 3)\n",
      "0.9501756\n",
      "[Epoch 2/3] [Batch 106/216] [D loss: 0.263640] [G loss: 5.189819] time: 0:05:18.695531\n",
      "(50, 128, 128, 3)\n",
      "0.90198404\n",
      "[Epoch 2/3] [Batch 107/216] [D loss: 0.264820] [G loss: 4.940777] time: 0:05:19.284831\n",
      "(50, 128, 128, 3)\n",
      "0.8846733\n",
      "[Epoch 2/3] [Batch 108/216] [D loss: 0.257146] [G loss: 5.628485] time: 0:05:19.871034\n",
      "(50, 128, 128, 3)\n",
      "0.90490127\n",
      "[Epoch 2/3] [Batch 109/216] [D loss: 0.260123] [G loss: 5.444755] time: 0:05:20.467196\n",
      "(50, 128, 128, 3)\n",
      "0.87421495\n",
      "[Epoch 2/3] [Batch 110/216] [D loss: 0.265202] [G loss: 4.496370] time: 0:05:21.074191\n",
      "(50, 128, 128, 3)\n",
      "0.88570285\n",
      "[Epoch 2/3] [Batch 111/216] [D loss: 0.266562] [G loss: 3.743915] time: 0:05:21.663012\n",
      "(50, 128, 128, 3)\n",
      "0.8719174\n",
      "[Epoch 2/3] [Batch 112/216] [D loss: 0.263802] [G loss: 4.697882] time: 0:05:22.263086\n",
      "(50, 128, 128, 3)\n",
      "0.9122346\n",
      "[Epoch 2/3] [Batch 113/216] [D loss: 0.266664] [G loss: 4.842690] time: 0:05:22.848431\n",
      "(50, 128, 128, 3)\n",
      "0.9157637\n",
      "[Epoch 2/3] [Batch 114/216] [D loss: 0.260515] [G loss: 6.288140] time: 0:05:23.443128\n",
      "(50, 128, 128, 3)\n",
      "0.9507056\n",
      "[Epoch 2/3] [Batch 115/216] [D loss: 0.255482] [G loss: 6.254693] time: 0:05:24.045195\n",
      "(50, 128, 128, 3)\n",
      "0.9170187\n",
      "[Epoch 2/3] [Batch 116/216] [D loss: 0.256197] [G loss: 4.752632] time: 0:05:24.625878\n",
      "(50, 128, 128, 3)\n",
      "0.8427219\n",
      "[Epoch 2/3] [Batch 117/216] [D loss: 0.256609] [G loss: 5.107956] time: 0:05:25.203251\n",
      "(50, 128, 128, 3)\n",
      "0.9091211\n",
      "[Epoch 2/3] [Batch 118/216] [D loss: 0.263002] [G loss: 4.748358] time: 0:05:25.797545\n",
      "(50, 128, 128, 3)\n",
      "0.9014666\n",
      "[Epoch 2/3] [Batch 119/216] [D loss: 0.253658] [G loss: 5.142195] time: 0:05:26.401182\n",
      "(50, 128, 128, 3)\n",
      "0.833813\n",
      "[Epoch 2/3] [Batch 120/216] [D loss: 0.259968] [G loss: 4.364368] time: 0:05:26.986782\n",
      "(50, 128, 128, 3)\n",
      "0.89311385\n",
      "[Epoch 2/3] [Batch 121/216] [D loss: 0.256223] [G loss: 6.119024] time: 0:05:27.586863\n",
      "(50, 128, 128, 3)\n",
      "0.86681026\n",
      "[Epoch 2/3] [Batch 122/216] [D loss: 0.258937] [G loss: 5.049810] time: 0:05:28.178039\n",
      "(50, 128, 128, 3)\n",
      "0.87553686\n",
      "[Epoch 2/3] [Batch 123/216] [D loss: 0.266188] [G loss: 4.352674] time: 0:05:28.758623\n",
      "(50, 128, 128, 3)\n",
      "0.8947155\n",
      "[Epoch 2/3] [Batch 124/216] [D loss: 0.265522] [G loss: 4.794425] time: 0:05:29.341882\n",
      "(50, 128, 128, 3)\n",
      "0.93011874\n",
      "[Epoch 2/3] [Batch 125/216] [D loss: 0.255170] [G loss: 4.961727] time: 0:05:29.941901\n",
      "(50, 128, 128, 3)\n",
      "0.86206174\n",
      "[Epoch 2/3] [Batch 126/216] [D loss: 0.258432] [G loss: 5.072472] time: 0:05:30.527913\n",
      "(50, 128, 128, 3)\n",
      "0.83557326\n",
      "[Epoch 2/3] [Batch 127/216] [D loss: 0.256780] [G loss: 4.890404] time: 0:05:31.150502\n",
      "(50, 128, 128, 3)\n",
      "0.86745113\n",
      "[Epoch 2/3] [Batch 128/216] [D loss: 0.262491] [G loss: 4.366861] time: 0:05:31.763186\n",
      "(50, 128, 128, 3)\n",
      "0.89353377\n",
      "[Epoch 2/3] [Batch 129/216] [D loss: 0.264448] [G loss: 4.233400] time: 0:05:32.350881\n",
      "(50, 128, 128, 3)\n",
      "0.83166474\n",
      "[Epoch 2/3] [Batch 130/216] [D loss: 0.261358] [G loss: 4.409019] time: 0:05:32.954122\n",
      "(50, 128, 128, 3)\n",
      "0.9433675\n",
      "[Epoch 2/3] [Batch 131/216] [D loss: 0.266139] [G loss: 4.214642] time: 0:05:33.544351\n",
      "(50, 128, 128, 3)\n",
      "0.8755016\n",
      "[Epoch 2/3] [Batch 132/216] [D loss: 0.260561] [G loss: 4.790558] time: 0:05:34.133293\n",
      "(50, 128, 128, 3)\n",
      "0.8214922\n",
      "[Epoch 2/3] [Batch 133/216] [D loss: 0.254079] [G loss: 4.828169] time: 0:05:34.726751\n",
      "(50, 128, 128, 3)\n",
      "0.8396588\n",
      "[Epoch 2/3] [Batch 134/216] [D loss: 0.266564] [G loss: 3.891355] time: 0:05:35.337662\n",
      "(50, 128, 128, 3)\n",
      "0.8956654\n",
      "[Epoch 2/3] [Batch 135/216] [D loss: 0.266230] [G loss: 5.240314] time: 0:05:35.928245\n",
      "(50, 128, 128, 3)\n",
      "0.88296366\n",
      "[Epoch 2/3] [Batch 136/216] [D loss: 0.267783] [G loss: 4.484506] time: 0:05:36.513749\n",
      "(50, 128, 128, 3)\n",
      "0.8851463\n",
      "[Epoch 2/3] [Batch 137/216] [D loss: 0.269089] [G loss: 4.417870] time: 0:05:37.106111\n",
      "(50, 128, 128, 3)\n",
      "0.9138646\n",
      "[Epoch 2/3] [Batch 138/216] [D loss: 0.257737] [G loss: 4.760626] time: 0:05:37.689934\n",
      "(50, 128, 128, 3)\n",
      "0.9047048\n",
      "[Epoch 2/3] [Batch 139/216] [D loss: 0.267527] [G loss: 5.424253] time: 0:05:38.296303\n",
      "(50, 128, 128, 3)\n",
      "0.9556815\n",
      "[Epoch 2/3] [Batch 140/216] [D loss: 0.264798] [G loss: 4.970775] time: 0:05:38.882574\n",
      "(50, 128, 128, 3)\n",
      "0.8888481\n",
      "[Epoch 2/3] [Batch 141/216] [D loss: 0.265121] [G loss: 4.953595] time: 0:05:39.473346\n",
      "(50, 128, 128, 3)\n",
      "0.88862807\n",
      "[Epoch 2/3] [Batch 142/216] [D loss: 0.264519] [G loss: 4.687533] time: 0:05:40.068938\n",
      "(50, 128, 128, 3)\n",
      "0.9130833\n",
      "[Epoch 2/3] [Batch 143/216] [D loss: 0.258066] [G loss: 5.069106] time: 0:05:40.654747\n",
      "(50, 128, 128, 3)\n",
      "0.90020436\n",
      "[Epoch 2/3] [Batch 144/216] [D loss: 0.254341] [G loss: 5.203415] time: 0:05:41.247513\n",
      "(50, 128, 128, 3)\n",
      "0.92690915\n",
      "[Epoch 2/3] [Batch 145/216] [D loss: 0.262292] [G loss: 4.771531] time: 0:05:41.826073\n",
      "(50, 128, 128, 3)\n",
      "0.89002794\n",
      "[Epoch 2/3] [Batch 146/216] [D loss: 0.262013] [G loss: 4.207221] time: 0:05:42.411200\n",
      "(50, 128, 128, 3)\n",
      "0.88590175\n",
      "[Epoch 2/3] [Batch 147/216] [D loss: 0.261308] [G loss: 5.394660] time: 0:05:42.999370\n",
      "(50, 128, 128, 3)\n",
      "0.9066823\n",
      "[Epoch 2/3] [Batch 148/216] [D loss: 0.255835] [G loss: 5.182946] time: 0:05:43.604491\n",
      "(50, 128, 128, 3)\n",
      "0.8491421\n",
      "[Epoch 2/3] [Batch 149/216] [D loss: 0.259497] [G loss: 4.317970] time: 0:05:44.222944\n",
      "(50, 128, 128, 3)\n",
      "0.8946748\n",
      "[Epoch 2/3] [Batch 150/216] [D loss: 0.261730] [G loss: 4.527256] time: 0:05:44.801863\n",
      "(50, 128, 128, 3)\n",
      "0.8622095\n",
      "[Epoch 2/3] [Batch 151/216] [D loss: 0.254752] [G loss: 5.190982] time: 0:05:45.406172\n",
      "(50, 128, 128, 3)\n",
      "0.8972013\n",
      "[Epoch 2/3] [Batch 152/216] [D loss: 0.253284] [G loss: 4.591310] time: 0:05:46.024971\n",
      "(50, 128, 128, 3)\n",
      "0.86150223\n",
      "[Epoch 2/3] [Batch 153/216] [D loss: 0.252712] [G loss: 5.115686] time: 0:05:46.647066\n",
      "(50, 128, 128, 3)\n",
      "0.8810664\n",
      "[Epoch 2/3] [Batch 154/216] [D loss: 0.261756] [G loss: 3.849761] time: 0:05:47.257238\n",
      "(50, 128, 128, 3)\n",
      "0.8751948\n",
      "[Epoch 2/3] [Batch 155/216] [D loss: 0.256163] [G loss: 4.949297] time: 0:05:47.853023\n",
      "(50, 128, 128, 3)\n",
      "0.906275\n",
      "[Epoch 2/3] [Batch 156/216] [D loss: 0.263091] [G loss: 4.210926] time: 0:05:48.429620\n",
      "(50, 128, 128, 3)\n",
      "0.8848999\n",
      "[Epoch 2/3] [Batch 157/216] [D loss: 0.256535] [G loss: 4.860025] time: 0:05:49.026851\n",
      "(50, 128, 128, 3)\n",
      "0.90057105\n",
      "[Epoch 2/3] [Batch 158/216] [D loss: 0.259638] [G loss: 5.346589] time: 0:05:49.619025\n",
      "(50, 128, 128, 3)\n",
      "0.87107116\n",
      "[Epoch 2/3] [Batch 159/216] [D loss: 0.261636] [G loss: 3.972056] time: 0:05:50.211694\n",
      "(50, 128, 128, 3)\n",
      "0.85417885\n",
      "[Epoch 2/3] [Batch 160/216] [D loss: 0.262167] [G loss: 4.026927] time: 0:05:50.808327\n",
      "(50, 128, 128, 3)\n",
      "0.91916555\n",
      "[Epoch 2/3] [Batch 161/216] [D loss: 0.257577] [G loss: 4.066953] time: 0:05:51.414807\n",
      "(50, 128, 128, 3)\n",
      "0.8602192\n",
      "[Epoch 2/3] [Batch 162/216] [D loss: 0.260730] [G loss: 4.426782] time: 0:05:52.018537\n",
      "(50, 128, 128, 3)\n",
      "0.8734004\n",
      "[Epoch 2/3] [Batch 163/216] [D loss: 0.261987] [G loss: 4.582883] time: 0:05:52.619931\n",
      "(50, 128, 128, 3)\n",
      "0.8911877\n",
      "[Epoch 2/3] [Batch 164/216] [D loss: 0.264428] [G loss: 4.224083] time: 0:05:53.240011\n",
      "(50, 128, 128, 3)\n",
      "0.90668005\n",
      "[Epoch 2/3] [Batch 165/216] [D loss: 0.256074] [G loss: 5.376787] time: 0:05:53.838358\n",
      "(50, 128, 128, 3)\n",
      "0.82238275\n",
      "[Epoch 2/3] [Batch 166/216] [D loss: 0.256336] [G loss: 4.511312] time: 0:05:54.439392\n",
      "(50, 128, 128, 3)\n",
      "0.8641603\n",
      "[Epoch 2/3] [Batch 167/216] [D loss: 0.263017] [G loss: 3.677555] time: 0:05:55.033686\n",
      "(50, 128, 128, 3)\n",
      "0.8823951\n",
      "[Epoch 2/3] [Batch 168/216] [D loss: 0.259090] [G loss: 4.650964] time: 0:05:55.622094\n",
      "(50, 128, 128, 3)\n",
      "0.90538603\n",
      "[Epoch 2/3] [Batch 169/216] [D loss: 0.257012] [G loss: 5.047823] time: 0:05:56.227704\n",
      "(50, 128, 128, 3)\n",
      "0.88419753\n",
      "[Epoch 2/3] [Batch 170/216] [D loss: 0.262709] [G loss: 4.682371] time: 0:05:56.835701\n",
      "(50, 128, 128, 3)\n",
      "0.8832793\n",
      "[Epoch 2/3] [Batch 171/216] [D loss: 0.265667] [G loss: 4.435994] time: 0:05:57.447232\n",
      "(50, 128, 128, 3)\n",
      "0.93276024\n",
      "[Epoch 2/3] [Batch 172/216] [D loss: 0.264421] [G loss: 4.587298] time: 0:05:58.059759\n",
      "(50, 128, 128, 3)\n",
      "0.9240527\n",
      "[Epoch 2/3] [Batch 173/216] [D loss: 0.259190] [G loss: 4.755880] time: 0:05:58.667326\n",
      "(50, 128, 128, 3)\n",
      "0.8463988\n",
      "[Epoch 2/3] [Batch 174/216] [D loss: 0.260217] [G loss: 4.681165] time: 0:05:59.261639\n",
      "(50, 128, 128, 3)\n",
      "0.90932804\n",
      "[Epoch 2/3] [Batch 175/216] [D loss: 0.264407] [G loss: 4.412714] time: 0:05:59.863355\n",
      "(50, 128, 128, 3)\n",
      "0.8974264\n",
      "[Epoch 2/3] [Batch 176/216] [D loss: 0.261353] [G loss: 5.349286] time: 0:06:00.484163\n",
      "(50, 128, 128, 3)\n",
      "0.85292274\n",
      "[Epoch 2/3] [Batch 177/216] [D loss: 0.260247] [G loss: 4.288184] time: 0:06:01.077960\n",
      "(50, 128, 128, 3)\n",
      "0.895101\n",
      "[Epoch 2/3] [Batch 178/216] [D loss: 0.252440] [G loss: 5.439602] time: 0:06:01.676176\n",
      "(50, 128, 128, 3)\n",
      "0.84213775\n",
      "[Epoch 2/3] [Batch 179/216] [D loss: 0.255019] [G loss: 5.338325] time: 0:06:02.279308\n",
      "(50, 128, 128, 3)\n",
      "0.88414335\n",
      "[Epoch 2/3] [Batch 180/216] [D loss: 0.263163] [G loss: 4.312424] time: 0:06:02.888872\n",
      "(50, 128, 128, 3)\n",
      "0.9228031\n",
      "[Epoch 2/3] [Batch 181/216] [D loss: 0.262071] [G loss: 4.479627] time: 0:06:03.473996\n",
      "(50, 128, 128, 3)\n",
      "0.91012865\n",
      "[Epoch 2/3] [Batch 182/216] [D loss: 0.249031] [G loss: 5.153283] time: 0:06:04.072624\n",
      "(50, 128, 128, 3)\n",
      "0.8425536\n",
      "[Epoch 2/3] [Batch 183/216] [D loss: 0.238499] [G loss: 4.666574] time: 0:06:04.665784\n",
      "(50, 128, 128, 3)\n",
      "0.9296959\n",
      "[Epoch 2/3] [Batch 184/216] [D loss: 0.259134] [G loss: 5.800147] time: 0:06:05.245668\n",
      "(50, 128, 128, 3)\n",
      "0.9370117\n",
      "[Epoch 2/3] [Batch 185/216] [D loss: 0.264680] [G loss: 4.019864] time: 0:06:05.834122\n",
      "(50, 128, 128, 3)\n",
      "0.8802078\n",
      "[Epoch 2/3] [Batch 186/216] [D loss: 0.257433] [G loss: 4.607744] time: 0:06:06.427907\n",
      "(50, 128, 128, 3)\n",
      "0.85704046\n",
      "[Epoch 2/3] [Batch 187/216] [D loss: 0.259902] [G loss: 4.727317] time: 0:06:07.026206\n",
      "(50, 128, 128, 3)\n",
      "0.953588\n",
      "[Epoch 2/3] [Batch 188/216] [D loss: 0.261044] [G loss: 4.572239] time: 0:06:07.638099\n",
      "(50, 128, 128, 3)\n",
      "0.8885169\n",
      "[Epoch 2/3] [Batch 189/216] [D loss: 0.259044] [G loss: 4.182266] time: 0:06:08.228569\n",
      "(50, 128, 128, 3)\n",
      "0.8612542\n",
      "[Epoch 2/3] [Batch 190/216] [D loss: 0.263725] [G loss: 3.862393] time: 0:06:08.820313\n",
      "(50, 128, 128, 3)\n",
      "0.91150814\n",
      "[Epoch 2/3] [Batch 191/216] [D loss: 0.262898] [G loss: 4.139315] time: 0:06:09.426063\n",
      "(50, 128, 128, 3)\n",
      "0.9053069\n",
      "[Epoch 2/3] [Batch 192/216] [D loss: 0.258098] [G loss: 5.866667] time: 0:06:10.050889\n",
      "(50, 128, 128, 3)\n",
      "0.8486459\n",
      "[Epoch 2/3] [Batch 193/216] [D loss: 0.260422] [G loss: 3.651926] time: 0:06:10.651399\n",
      "(50, 128, 128, 3)\n",
      "0.89059496\n",
      "[Epoch 2/3] [Batch 194/216] [D loss: 0.259723] [G loss: 4.304668] time: 0:06:11.251977\n",
      "(50, 128, 128, 3)\n",
      "0.91363573\n",
      "[Epoch 2/3] [Batch 195/216] [D loss: 0.257289] [G loss: 4.066968] time: 0:06:11.847846\n",
      "(50, 128, 128, 3)\n",
      "0.89008695\n",
      "[Epoch 2/3] [Batch 196/216] [D loss: 0.264280] [G loss: 4.639191] time: 0:06:12.461994\n",
      "(50, 128, 128, 3)\n",
      "0.9381428\n",
      "[Epoch 2/3] [Batch 197/216] [D loss: 0.258359] [G loss: 4.407341] time: 0:06:13.051845\n",
      "(50, 128, 128, 3)\n",
      "0.8875143\n",
      "[Epoch 2/3] [Batch 198/216] [D loss: 0.256738] [G loss: 4.546998] time: 0:06:13.642696\n",
      "(50, 128, 128, 3)\n",
      "0.84549665\n",
      "[Epoch 2/3] [Batch 199/216] [D loss: 0.263258] [G loss: 4.217856] time: 0:06:14.235067\n",
      "(50, 128, 128, 3)\n",
      "0.9161561\n",
      "[Epoch 2/3] [Batch 200/216] [D loss: 0.259998] [G loss: 4.795862] time: 0:06:14.849254\n",
      "(50, 128, 128, 3)\n",
      "0.9253314\n",
      "[Epoch 2/3] [Batch 201/216] [D loss: 0.256837] [G loss: 4.640842] time: 0:06:15.459223\n",
      "(50, 128, 128, 3)\n",
      "0.8380162\n",
      "[Epoch 2/3] [Batch 202/216] [D loss: 0.255828] [G loss: 4.873796] time: 0:06:16.068839\n",
      "(50, 128, 128, 3)\n",
      "0.8785251\n",
      "[Epoch 2/3] [Batch 203/216] [D loss: 0.259653] [G loss: 4.378284] time: 0:06:16.692712\n",
      "(50, 128, 128, 3)\n",
      "0.87276167\n",
      "[Epoch 2/3] [Batch 204/216] [D loss: 0.262672] [G loss: 3.547539] time: 0:06:17.307180\n",
      "(50, 128, 128, 3)\n",
      "0.90781206\n",
      "[Epoch 2/3] [Batch 205/216] [D loss: 0.257888] [G loss: 4.361653] time: 0:06:17.894163\n",
      "(50, 128, 128, 3)\n",
      "0.7914807\n",
      "[Epoch 2/3] [Batch 206/216] [D loss: 0.252109] [G loss: 5.407523] time: 0:06:18.495246\n",
      "(50, 128, 128, 3)\n",
      "0.8421745\n",
      "[Epoch 2/3] [Batch 207/216] [D loss: 0.266797] [G loss: 3.556666] time: 0:06:19.079508\n",
      "(50, 128, 128, 3)\n",
      "0.78333807\n",
      "[Epoch 2/3] [Batch 208/216] [D loss: 0.250004] [G loss: 5.990637] time: 0:06:19.668072\n",
      "(50, 128, 128, 3)\n",
      "0.81805515\n",
      "[Epoch 2/3] [Batch 209/216] [D loss: 0.255832] [G loss: 4.435533] time: 0:06:20.266592\n",
      "(50, 128, 128, 3)\n",
      "0.8409946\n",
      "[Epoch 2/3] [Batch 210/216] [D loss: 0.261026] [G loss: 3.329081] time: 0:06:20.875997\n",
      "(50, 128, 128, 3)\n",
      "0.852175\n",
      "[Epoch 2/3] [Batch 211/216] [D loss: 0.257922] [G loss: 5.181689] time: 0:06:21.448049\n",
      "(50, 128, 128, 3)\n",
      "0.8574832\n",
      "[Epoch 2/3] [Batch 212/216] [D loss: 0.253963] [G loss: 4.832808] time: 0:06:22.044837\n",
      "(50, 128, 128, 3)\n",
      "0.84304214\n",
      "[Epoch 2/3] [Batch 213/216] [D loss: 0.255740] [G loss: 3.389042] time: 0:06:22.632134\n",
      "(50, 128, 128, 3)\n",
      "0.8081719\n",
      "[Epoch 2/3] [Batch 214/216] [D loss: 0.246083] [G loss: 6.021075] time: 0:06:23.224858\n",
      "(50, 128, 128, 3)\n",
      "0.8554823\n",
      "[Epoch 2/3] [Batch 215/216] [D loss: 0.257738] [G loss: 4.417139] time: 0:06:23.815854\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline\n",
    "epochs = 3#cfg.NUM_EPOCHS\n",
    "\n",
    "test_first_imgs, test_last_imgs = next(test_batch_generator)\n",
    "steps_per_epoch = (nbr_train_data // cfg.BATCH_SIZE) \n",
    "print(steps_per_epoch)\n",
    "g_loss_plot = np.zeros((steps_per_epoch,epochs))\n",
    "d_loss_plot = np.zeros((steps_per_epoch,epochs))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch_i in range(steps_per_epoch):\n",
    "        first_frames, last_frames= next(train_batch_generator)\n",
    "        if first_frames.shape[0] == cfg.BATCH_SIZE: \n",
    "             \n",
    "            fake_last_frames = modelObj.generator.predict(first_frames)\n",
    "\n",
    "            d_loss_real = modelObj.discriminator.train_on_batch([last_frames, first_frames], valid)\n",
    "            d_loss_fake = modelObj.discriminator.train_on_batch([fake_last_frames, first_frames], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            d_loss_plot[batch_i,epoch] = d_loss[0]\n",
    "            \n",
    "            # Train the generator\n",
    "            g_loss = modelObj.combined.train_on_batch([last_frames, first_frames], [valid, last_frames])\n",
    "            g_loss_plot[batch_i,epoch] = g_loss[0]\n",
    "            elapsed_time = datetime.now() - start_time \n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                                               batch_i,\n",
    "                                                                                               steps_per_epoch,\n",
    "                                                                                               d_loss[0], \n",
    "                                                                                               g_loss[0],\n",
    "                                                                                               elapsed_time))\n",
    "            # run some tests to check how the generated images evolve during training\n",
    "            test_fake_last_imgs = modelObj.generator.predict(test_first_imgs)\n",
    "            test_img_name = output_log_dir + \"/gen_img_epoc_\" + str(epoch) + \".png\"\n",
    "            merged_img = np.vstack((first_frames[0],last_frames[0],fake_last_frames[0]))\n",
    "            imageio.imwrite(test_img_name, img_as_ubyte(merged_img)) #scipy.misc.imsave(test_img_name, merged_img)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate images\n",
    "\n",
    "for batch_i in range(100):\n",
    "    test_first_imgs, test_last_imgs = next(test_batch_generator)\n",
    "    test_fake_last_imgs = modelObj.generator.predict(test_first_imgs) \n",
    "    test_img_name = output_log_dir + \"/gen_img_test_\" + str(batch_i) + \".png\"\n",
    "    merged_img = np.vstack((test_first_imgs[0],test_last_imgs[0],test_fake_last_imgs[0]))\n",
    "    imageio.imwrite(test_img_name, img_as_ubyte(merged_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6418356895446777, 0.2725187838077545, 0.02369316853582859]\n"
     ]
    }
   ],
   "source": [
    "##g_loss_plot\n",
    "print(g_loss)\n",
    "steps=216\n",
    "num_of_epochs = 3\n",
    "g_loss_vector_plot = np.zeros(steps*num_of_epochs)\n",
    "d_loss_vector_plot = np.zeros(steps*num_of_epochs)\n",
    "#print(d_loss_plot)\n",
    "for i in range(num_of_epochs):\n",
    "    for k in range(steps):\n",
    "        g_loss_vector_plot[k+i*steps] = g_loss_plot[k,i]\n",
    "        d_loss_vector_plot[k+i*steps] = d_loss_plot[k,i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(d_loss_vector_plot)\n",
    "plt.title('D loss')\n",
    "plt.ylabel('D loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_loss_vector_plot)\n",
    "plt.title('Generator loss')\n",
    "plt.ylabel('G loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
